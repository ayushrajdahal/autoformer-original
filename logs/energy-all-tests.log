[Running] bash "/home/hpc/Documents/autoformer-original/scripts/Energy_script/all_tests.sh"
2025-02-11:22:37:02,910 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:22:37:02,910 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='2_1_default', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=24, bucket_size=4, n_hashes=4, enc_in=16, dec_in=16, c_out=16, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_yrs=3, val_yrs=1, train_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 2_1_default_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8521
val 8617
test 25897
	iters: 100, epoch: 1 | loss: 0.4237661
	speed: 0.0423s/iter; left time: 74.6591s
	iters: 200, epoch: 1 | loss: 0.4075328
	speed: 0.0394s/iter; left time: 65.5854s
Epoch: 1 cost time: 10.87197732925415
Epoch: 1, Steps: 266 | Train Loss: 0.4543977 Vali Loss: 0.4172949 Test Loss: 0.4616160
Validation loss decreased (inf --> 0.417295).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.3115965
	speed: 0.2828s/iter; left time: 423.3298s
	iters: 200, epoch: 2 | loss: 0.3370855
	speed: 0.0366s/iter; left time: 51.1683s
Epoch: 2 cost time: 10.08568525314331
Epoch: 2, Steps: 266 | Train Loss: 0.3337937 Vali Loss: 0.4551221 Test Loss: 0.5028363
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.3817741
	speed: 0.2840s/iter; left time: 349.5820s
	iters: 200, epoch: 3 | loss: 0.2550570
	speed: 0.0363s/iter; left time: 41.0868s
Epoch: 3 cost time: 10.00369381904602
Epoch: 3, Steps: 266 | Train Loss: 0.2680657 Vali Loss: 0.4567428 Test Loss: 0.5123010
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.2286854
	speed: 0.2881s/iter; left time: 277.9729s
	iters: 200, epoch: 4 | loss: 0.2223253
	speed: 0.0371s/iter; left time: 32.0589s
Epoch: 4 cost time: 10.384636402130127
Epoch: 4, Steps: 266 | Train Loss: 0.2383952 Vali Loss: 0.4610199 Test Loss: 0.5188524
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : 2_1_default_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 25897
test shape: (25897, 24, 16) (25897, 24, 16)
test shape: (25897, 24, 16) (25897, 24, 16)
model_id: 2_1_default, mse:0.46162301301956177, mae:0.46021056175231934, rmse:0.6794284582138062 mape:2.86728835105896 mspe:10445.25
2025-02-11:22:39:35,841 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:22:39:35,841 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='2_4_48seqlen', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=48, pred_len=24, bucket_size=4, n_hashes=4, enc_in=16, dec_in=16, c_out=16, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_yrs=3, val_yrs=1, train_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 2_4_48seqlen_Autoformer_Energy_ftM_sl48_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8569
val 8617
test 25897
	iters: 100, epoch: 1 | loss: 0.5368854
	speed: 0.0388s/iter; left time: 68.6004s
	iters: 200, epoch: 1 | loss: 0.3603112
	speed: 0.0339s/iter; left time: 56.6557s
Epoch: 1 cost time: 9.60017991065979
Epoch: 1, Steps: 267 | Train Loss: 0.4308545 Vali Loss: 0.4166284 Test Loss: 0.4620884
Validation loss decreased (inf --> 0.416628).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.3005643
	speed: 0.2320s/iter; left time: 348.7042s
	iters: 200, epoch: 2 | loss: 0.3022215
	speed: 0.0347s/iter; left time: 48.6362s

[Done] exited with code=null in 191.472 seconds

[Running] bash "/home/hpc/Documents/autoformer-original/scripts/Energy_script/all_tests.sh"
2025-02-11:22:44:59,026 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:22:44:59,026 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='2_1_default', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=24, bucket_size=4, n_hashes=4, enc_in=16, dec_in=16, c_out=16, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=3, val_yrs=1, test_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 2_1_default_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 25801
val 8617
test 8617
	iters: 100, epoch: 1 | loss: 0.3836456
	speed: 0.0419s/iter; left time: 232.1054s
	iters: 200, epoch: 1 | loss: 0.3446593
	speed: 0.0369s/iter; left time: 200.5776s
	iters: 300, epoch: 1 | loss: 0.3084956
	speed: 0.0367s/iter; left time: 196.0688s
	iters: 400, epoch: 1 | loss: 0.3661915
	speed: 0.0371s/iter; left time: 194.3423s
	iters: 500, epoch: 1 | loss: 0.2890608
	speed: 0.0363s/iter; left time: 186.7143s
	iters: 600, epoch: 1 | loss: 0.3308579
	speed: 0.0381s/iter; left time: 191.9902s
	iters: 700, epoch: 1 | loss: 0.3376904
	speed: 0.0363s/iter; left time: 179.4231s
	iters: 800, epoch: 1 | loss: 0.2953833
	speed: 0.0370s/iter; left time: 179.2862s
Epoch: 1 cost time: 30.34158730506897
Epoch: 1, Steps: 806 | Train Loss: 0.3970004 Vali Loss: 0.3569956 Test Loss: 0.3732619
Validation loss decreased (inf --> 0.356996).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.3045684
	speed: 0.1536s/iter; left time: 727.5647s
	iters: 200, epoch: 2 | loss: 0.3216138
	speed: 0.0360s/iter; left time: 167.1596s
	iters: 300, epoch: 2 | loss: 0.3215429
	speed: 0.0354s/iter; left time: 160.5957s
	iters: 400, epoch: 2 | loss: 0.2644075
	speed: 0.0355s/iter; left time: 157.4376s
	iters: 500, epoch: 2 | loss: 0.2117894
	speed: 0.0362s/iter; left time: 157.0997s
	iters: 600, epoch: 2 | loss: 0.2946810
	speed: 0.0363s/iter; left time: 153.6288s
	iters: 700, epoch: 2 | loss: 0.2605491
	speed: 0.0361s/iter; left time: 149.2629s
	iters: 800, epoch: 2 | loss: 0.3669419
	speed: 0.0360s/iter; left time: 145.3482s
Epoch: 2 cost time: 29.243173599243164
Epoch: 2, Steps: 806 | Train Loss: 0.2972580 Vali Loss: 0.3602186 Test Loss: 0.3835632
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.2407528
	speed: 0.1558s/iter; left time: 612.3074s
	iters: 200, epoch: 3 | loss: 0.2516048
	speed: 0.0410s/iter; left time: 157.1993s
	iters: 300, epoch: 3 | loss: 0.2293020
	speed: 0.0414s/iter; left time: 154.6101s
	iters: 400, epoch: 3 | loss: 0.2167091
	speed: 0.0395s/iter; left time: 143.3035s
	iters: 500, epoch: 3 | loss: 0.2778401
	speed: 0.0360s/iter; left time: 127.0139s
	iters: 600, epoch: 3 | loss: 0.2158907
	speed: 0.0403s/iter; left time: 138.2601s
	iters: 700, epoch: 3 | loss: 0.2898200
	speed: 0.0376s/iter; left time: 125.3511s
	iters: 800, epoch: 3 | loss: 0.2768560
	speed: 0.0359s/iter; left time: 115.9430s
Epoch: 3 cost time: 31.68881320953369
Epoch: 3, Steps: 806 | Train Loss: 0.2414739 Vali Loss: 0.3751440 Test Loss: 0.3906024
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.1943086
	speed: 0.1524s/iter; left time: 476.3329s
	iters: 200, epoch: 4 | loss: 0.2853668
	speed: 0.0391s/iter; left time: 118.1942s
	iters: 300, epoch: 4 | loss: 0.1806853
	speed: 0.0359s/iter; left time: 105.1537s
	iters: 400, epoch: 4 | loss: 0.2198511
	speed: 0.0358s/iter; left time: 101.1524s
	iters: 500, epoch: 4 | loss: 0.1995637
	speed: 0.0352s/iter; left time: 95.9976s
	iters: 600, epoch: 4 | loss: 0.2011287
	speed: 0.0362s/iter; left time: 95.1286s
	iters: 700, epoch: 4 | loss: 0.1786178
	speed: 0.0362s/iter; left time: 91.3795s
	iters: 800, epoch: 4 | loss: 0.2259350
	speed: 0.0361s/iter; left time: 87.6407s
Epoch: 4 cost time: 29.628260135650635
Epoch: 4, Steps: 806 | Train Loss: 0.2143566 Vali Loss: 0.3790588 Test Loss: 0.3955117
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : 2_1_default_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 8617
test shape: (8617, 24, 16) (8617, 24, 16)
test shape: (8617, 24, 16) (8617, 24, 16)
model_id: 2_1_default, mse:0.37349846959114075, mae:0.4035469591617584, rmse:0.6111451983451843 mape:2.6028926372528076 mspe:4639.81298828125
2025-02-11:22:47:55,474 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:22:47:55,474 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='2_4_48seqlen', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=48, pred_len=24, bucket_size=4, n_hashes=4, enc_in=16, dec_in=16, c_out=16, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=3, val_yrs=1, test_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 2_4_48seqlen_Autoformer_Energy_ftM_sl48_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 25849
val 8617
test 8617
	iters: 100, epoch: 1 | loss: 0.4882161
	speed: 0.0467s/iter; left time: 258.9921s
	iters: 200, epoch: 1 | loss: 0.4461271
	speed: 0.0391s/iter; left time: 213.0952s
	iters: 300, epoch: 1 | loss: 0.3422178
	speed: 0.0379s/iter; left time: 202.5324s
	iters: 400, epoch: 1 | loss: 0.2672193
	speed: 0.0331s/iter; left time: 173.7838s
	iters: 500, epoch: 1 | loss: 0.3293242
	speed: 0.0333s/iter; left time: 171.5144s
	iters: 600, epoch: 1 | loss: 0.3281721
	speed: 0.0366s/iter; left time: 185.0265s
	iters: 700, epoch: 1 | loss: 0.3007031
	speed: 0.0337s/iter; left time: 166.5698s
	iters: 800, epoch: 1 | loss: 0.2685177
	speed: 0.0350s/iter; left time: 169.6151s
Epoch: 1 cost time: 29.816868543624878
Epoch: 1, Steps: 807 | Train Loss: 0.3779872 Vali Loss: 0.3295854 Test Loss: 0.3398633
Validation loss decreased (inf --> 0.329585).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.3305566
	speed: 0.1340s/iter; left time: 635.5008s
	iters: 200, epoch: 2 | loss: 0.3146860
	speed: 0.0360s/iter; left time: 167.1209s
	iters: 300, epoch: 2 | loss: 0.2717746
	speed: 0.0342s/iter; left time: 155.5676s
	iters: 400, epoch: 2 | loss: 0.3260231
	speed: 0.0385s/iter; left time: 171.0676s
	iters: 500, epoch: 2 | loss: 0.2931502
	speed: 0.0376s/iter; left time: 163.3745s
	iters: 600, epoch: 2 | loss: 0.2829516
	speed: 0.0336s/iter; left time: 142.5201s
	iters: 700, epoch: 2 | loss: 0.2910203
	speed: 0.0381s/iter; left time: 157.6961s
	iters: 800, epoch: 2 | loss: 0.2821236
	speed: 0.0360s/iter; left time: 145.3998s
Epoch: 2 cost time: 29.82567000389099
Epoch: 2, Steps: 807 | Train Loss: 0.3029031 Vali Loss: 0.3085633 Test Loss: 0.3321525
Validation loss decreased (0.329585 --> 0.308563).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.3081831
	speed: 0.1333s/iter; left time: 524.6477s
	iters: 200, epoch: 3 | loss: 0.3547806
	speed: 0.0329s/iter; left time: 126.0545s
	iters: 300, epoch: 3 | loss: 0.2648221
	speed: 0.0332s/iter; left time: 124.1716s
	iters: 400, epoch: 3 | loss: 0.2518761
	speed: 0.0347s/iter; left time: 126.3201s
	iters: 500, epoch: 3 | loss: 0.2116068
	speed: 0.0371s/iter; left time: 131.1011s
	iters: 600, epoch: 3 | loss: 0.3583733
	speed: 0.0382s/iter; left time: 131.1552s
	iters: 700, epoch: 3 | loss: 0.2131499
	speed: 0.0378s/iter; left time: 126.0611s
	iters: 800, epoch: 3 | loss: 0.2397951
	speed: 0.0357s/iter; left time: 115.5315s
Epoch: 3 cost time: 29.332407474517822
Epoch: 3, Steps: 807 | Train Loss: 0.2640091 Vali Loss: 0.3027198 Test Loss: 0.3279531
Validation loss decreased (0.308563 --> 0.302720).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.2518023
	speed: 0.1288s/iter; left time: 403.1696s
	iters: 200, epoch: 4 | loss: 0.2041159
	speed: 0.0332s/iter; left time: 100.5609s
	iters: 300, epoch: 4 | loss: 0.3563827
	speed: 0.0389s/iter; left time: 114.0577s
	iters: 400, epoch: 4 | loss: 0.2079272
	speed: 0.0335s/iter; left time: 94.6852s
	iters: 500, epoch: 4 | loss: 0.2069556
	speed: 0.0346s/iter; left time: 94.3304s
	iters: 600, epoch: 4 | loss: 0.2439553
	speed: 0.0332s/iter; left time: 87.2103s
	iters: 700, epoch: 4 | loss: 0.2162548
	speed: 0.0332s/iter; left time: 83.9263s
	iters: 800, epoch: 4 | loss: 0.2755077
	speed: 0.0331s/iter; left time: 80.3893s
Epoch: 4 cost time: 27.91608214378357
Epoch: 4, Steps: 807 | Train Loss: 0.2416258 Vali Loss: 0.3084370 Test Loss: 0.3389323
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.2021549
	speed: 0.1308s/iter; left time: 303.6883s
	iters: 200, epoch: 5 | loss: 0.2173322
	speed: 0.0341s/iter; left time: 75.7134s
	iters: 300, epoch: 5 | loss: 0.2609439
	speed: 0.0332s/iter; left time: 70.5414s
	iters: 400, epoch: 5 | loss: 0.2492839
	speed: 0.0328s/iter; left time: 66.3318s
	iters: 500, epoch: 5 | loss: 0.1809879
	speed: 0.0330s/iter; left time: 63.4600s
	iters: 600, epoch: 5 | loss: 0.2686653
	speed: 0.0330s/iter; left time: 60.0414s
	iters: 700, epoch: 5 | loss: 0.2473572
	speed: 0.0333s/iter; left time: 57.2857s
	iters: 800, epoch: 5 | loss: 0.2068185
	speed: 0.0369s/iter; left time: 59.7772s
Epoch: 5 cost time: 27.88489532470703
Epoch: 5, Steps: 807 | Train Loss: 0.2294581 Vali Loss: 0.3097919 Test Loss: 0.3454456
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.1964340
	speed: 0.1290s/iter; left time: 195.4867s
	iters: 200, epoch: 6 | loss: 0.1904037
	speed: 0.0327s/iter; left time: 46.2454s
	iters: 300, epoch: 6 | loss: 0.1772326
	speed: 0.0323s/iter; left time: 42.4837s
	iters: 400, epoch: 6 | loss: 0.1975791
	speed: 0.0324s/iter; left time: 39.3301s
	iters: 500, epoch: 6 | loss: 0.1864269
	speed: 0.0333s/iter; left time: 37.0906s
	iters: 600, epoch: 6 | loss: 0.2119930
	speed: 0.0349s/iter; left time: 35.4509s
	iters: 700, epoch: 6 | loss: 0.1941290
	speed: 0.0330s/iter; left time: 30.1781s
	iters: 800, epoch: 6 | loss: 0.2015159
	speed: 0.0331s/iter; left time: 26.9462s
Epoch: 6 cost time: 26.977742671966553
Epoch: 6, Steps: 807 | Train Loss: 0.2230881 Vali Loss: 0.3083229 Test Loss: 0.3434956
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : 2_4_48seqlen_Autoformer_Energy_ftM_sl48_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 8617
test shape: (8617, 24, 16) (8617, 24, 16)
test shape: (8617, 24, 16) (8617, 24, 16)
model_id: 2_4_48seqlen, mse:0.3281294107437134, mae:0.3709026873111725, rmse:0.5728257894515991 mape:2.6051697731018066 mspe:7200.61328125
2025-02-11:22:51:50,426 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:22:51:50,426 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='2_4_144seqlen', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=144, label_len=48, pred_len=24, bucket_size=4, n_hashes=4, enc_in=16, dec_in=16, c_out=16, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=3, val_yrs=1, test_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 2_4_144seqlen_Autoformer_Energy_ftM_sl144_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 25753
val 8617
test 8617
	iters: 100, epoch: 1 | loss: 0.4150479
	speed: 0.0527s/iter; left time: 291.2753s
	iters: 200, epoch: 1 | loss: 0.4355840
	speed: 0.0461s/iter; left time: 250.2185s
	iters: 300, epoch: 1 | loss: 0.4223354
	speed: 0.0458s/iter; left time: 244.1906s
	iters: 400, epoch: 1 | loss: 0.3824932
	speed: 0.0478s/iter; left time: 250.0755s
	iters: 500, epoch: 1 | loss: 0.3241286
	speed: 0.0457s/iter; left time: 234.2772s
	iters: 600, epoch: 1 | loss: 0.3731987
	speed: 0.0453s/iter; left time: 228.0219s
	iters: 700, epoch: 1 | loss: 0.2983749
	speed: 0.0434s/iter; left time: 213.8913s
	iters: 800, epoch: 1 | loss: 0.3432888
	speed: 0.0475s/iter; left time: 229.4776s
Epoch: 1 cost time: 37.693496227264404
Epoch: 1, Steps: 804 | Train Loss: 0.4006958 Vali Loss: 0.3989853 Test Loss: 0.4153172
Validation loss decreased (inf --> 0.398985).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.2794102
	speed: 0.1906s/iter; left time: 900.7242s
	iters: 200, epoch: 2 | loss: 0.2656519
	speed: 0.0447s/iter; left time: 206.5445s
	iters: 300, epoch: 2 | loss: 0.2664152
	speed: 0.0445s/iter; left time: 201.5591s
	iters: 400, epoch: 2 | loss: 0.3072899
	speed: 0.0434s/iter; left time: 192.1848s
	iters: 500, epoch: 2 | loss: 0.3138371
	speed: 0.0433s/iter; left time: 187.3021s
	iters: 600, epoch: 2 | loss: 0.2794298
	speed: 0.0435s/iter; left time: 183.7136s
	iters: 700, epoch: 2 | loss: 0.2844926
	speed: 0.0467s/iter; left time: 192.5977s
	iters: 800, epoch: 2 | loss: 0.2508211
	speed: 0.0456s/iter; left time: 183.3418s
Epoch: 2 cost time: 36.20201754570007
Epoch: 2, Steps: 804 | Train Loss: 0.2902093 Vali Loss: 0.3840365 Test Loss: 0.3995014
Validation loss decreased (0.398985 --> 0.384037).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.2401719
	speed: 0.1882s/iter; left time: 738.0979s
	iters: 200, epoch: 3 | loss: 0.2349801
	speed: 0.0478s/iter; left time: 182.5417s
	iters: 300, epoch: 3 | loss: 0.2283525
	speed: 0.0478s/iter; left time: 177.8084s
	iters: 400, epoch: 3 | loss: 0.2169916
	speed: 0.0492s/iter; left time: 178.1287s
	iters: 500, epoch: 3 | loss: 0.2525487
	speed: 0.0467s/iter; left time: 164.4310s
	iters: 600, epoch: 3 | loss: 0.2175760
	speed: 0.0442s/iter; left time: 151.2090s
	iters: 700, epoch: 3 | loss: 0.2141995
	speed: 0.0435s/iter; left time: 144.5944s
	iters: 800, epoch: 3 | loss: 0.1818382
	speed: 0.0446s/iter; left time: 143.8105s
Epoch: 3 cost time: 37.4836802482605
Epoch: 3, Steps: 804 | Train Loss: 0.2314548 Vali Loss: 0.3873146 Test Loss: 0.4065725
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.1934509
	speed: 0.1879s/iter; left time: 585.7927s
	iters: 200, epoch: 4 | loss: 0.2425697
	speed: 0.0445s/iter; left time: 134.2755s
	iters: 300, epoch: 4 | loss: 0.2002012
	speed: 0.0449s/iter; left time: 130.8514s
	iters: 400, epoch: 4 | loss: 0.2085799
	speed: 0.0461s/iter; left time: 129.7880s
	iters: 500, epoch: 4 | loss: 0.2187669
	speed: 0.0458s/iter; left time: 124.4464s
	iters: 600, epoch: 4 | loss: 0.2083196
	speed: 0.0439s/iter; left time: 114.8912s
	iters: 700, epoch: 4 | loss: 0.2009873
	speed: 0.0438s/iter; left time: 110.1646s
	iters: 800, epoch: 4 | loss: 0.1954536
	speed: 0.0457s/iter; left time: 110.5144s
Epoch: 4 cost time: 36.5087525844574
Epoch: 4, Steps: 804 | Train Loss: 0.2036917 Vali Loss: 0.3882851 Test Loss: 0.4174570
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.1771318
	speed: 0.1852s/iter; left time: 428.4448s
	iters: 200, epoch: 5 | loss: 0.1733018
	speed: 0.0464s/iter; left time: 102.6357s
	iters: 300, epoch: 5 | loss: 0.2223186
	speed: 0.0462s/iter; left time: 97.5647s
	iters: 400, epoch: 5 | loss: 0.1804110
	speed: 0.0437s/iter; left time: 87.9391s
	iters: 500, epoch: 5 | loss: 0.1995412
	speed: 0.0471s/iter; left time: 90.1836s
	iters: 600, epoch: 5 | loss: 0.1601970
	speed: 0.0441s/iter; left time: 79.9824s
	iters: 700, epoch: 5 | loss: 0.2068930
	speed: 0.0435s/iter; left time: 74.4874s
	iters: 800, epoch: 5 | loss: 0.1601156
	speed: 0.0445s/iter; left time: 71.7329s
Epoch: 5 cost time: 36.39744257926941
Epoch: 5, Steps: 804 | Train Loss: 0.1906247 Vali Loss: 0.3914568 Test Loss: 0.4168148
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : 2_4_144seqlen_Autoformer_Energy_ftM_sl144_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 8617
test shape: (8617, 24, 16) (8617, 24, 16)
test shape: (8617, 24, 16) (8617, 24, 16)
model_id: 2_4_144seqlen, mse:0.39981597661972046, mae:0.4325743615627289, rmse:0.6323100328445435 mape:2.7518863677978516 mspe:6841.072265625
2025-02-11:22:56:15,594 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:22:56:15,594 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='2_5_24labellen', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=24, pred_len=24, bucket_size=4, n_hashes=4, enc_in=16, dec_in=16, c_out=16, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=3, val_yrs=1, test_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 2_5_24labellen_Autoformer_Energy_ftM_sl96_ll24_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 25801
val 8617
test 8617
	iters: 100, epoch: 1 | loss: 0.3749483
	speed: 0.0411s/iter; left time: 227.9298s
	iters: 200, epoch: 1 | loss: 0.3465340
	speed: 0.0354s/iter; left time: 192.5960s
	iters: 300, epoch: 1 | loss: 0.2877943
	speed: 0.0355s/iter; left time: 189.8803s
	iters: 400, epoch: 1 | loss: 0.3513577
	speed: 0.0345s/iter; left time: 180.7699s
	iters: 500, epoch: 1 | loss: 0.2797787
	speed: 0.0339s/iter; left time: 174.5647s
	iters: 600, epoch: 1 | loss: 0.3223068
	speed: 0.0347s/iter; left time: 175.0651s
	iters: 700, epoch: 1 | loss: 0.3251654
	speed: 0.0360s/iter; left time: 177.9671s
	iters: 800, epoch: 1 | loss: 0.2705385
	speed: 0.0347s/iter; left time: 168.2921s
Epoch: 1 cost time: 28.86204195022583
Epoch: 1, Steps: 806 | Train Loss: 0.3794348 Vali Loss: 0.3279175 Test Loss: 0.3454073
Validation loss decreased (inf --> 0.327917).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.2670823
	speed: 0.1466s/iter; left time: 694.6649s
	iters: 200, epoch: 2 | loss: 0.2812414
	speed: 0.0355s/iter; left time: 164.4257s
	iters: 300, epoch: 2 | loss: 0.3169618
	speed: 0.0355s/iter; left time: 160.9430s
	iters: 400, epoch: 2 | loss: 0.2817224
	speed: 0.0355s/iter; left time: 157.4038s
	iters: 500, epoch: 2 | loss: 0.1925783
	speed: 0.0355s/iter; left time: 154.1470s
	iters: 600, epoch: 2 | loss: 0.2844588
	speed: 0.0359s/iter; left time: 151.9176s
	iters: 700, epoch: 2 | loss: 0.2711490
	speed: 0.0355s/iter; left time: 147.0389s
	iters: 800, epoch: 2 | loss: 0.3547955
	speed: 0.0357s/iter; left time: 144.0980s
Epoch: 2 cost time: 29.037192583084106
Epoch: 2, Steps: 806 | Train Loss: 0.2868828 Vali Loss: 0.3394524 Test Loss: 0.3601143
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.2282000
	speed: 0.1456s/iter; left time: 572.2875s
	iters: 200, epoch: 3 | loss: 0.2514969
	speed: 0.0356s/iter; left time: 136.2010s
	iters: 300, epoch: 3 | loss: 0.2002541
	speed: 0.0383s/iter; left time: 142.8361s
	iters: 400, epoch: 3 | loss: 0.2104415
	speed: 0.0377s/iter; left time: 136.7248s
	iters: 500, epoch: 3 | loss: 0.2697515
	speed: 0.0360s/iter; left time: 127.2692s
	iters: 600, epoch: 3 | loss: 0.2256975
	speed: 0.0359s/iter; left time: 123.2718s
	iters: 700, epoch: 3 | loss: 0.3034261
	speed: 0.0356s/iter; left time: 118.4475s
	iters: 800, epoch: 3 | loss: 0.2912145
	speed: 0.0386s/iter; left time: 124.7152s
Epoch: 3 cost time: 29.99258852005005
Epoch: 3, Steps: 806 | Train Loss: 0.2391602 Vali Loss: 0.3459054 Test Loss: 0.3640921
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.1859244
	speed: 0.1443s/iter; left time: 451.0571s
	iters: 200, epoch: 4 | loss: 0.2772875
	speed: 0.0359s/iter; left time: 108.6749s
	iters: 300, epoch: 4 | loss: 0.1863804
	speed: 0.0356s/iter; left time: 104.1424s
	iters: 400, epoch: 4 | loss: 0.2027299
	speed: 0.0357s/iter; left time: 100.9421s
	iters: 500, epoch: 4 | loss: 0.1990909
	speed: 0.0359s/iter; left time: 97.7862s
	iters: 600, epoch: 4 | loss: 0.1937065
	speed: 0.0357s/iter; left time: 93.6423s
	iters: 700, epoch: 4 | loss: 0.1775581
	speed: 0.0355s/iter; left time: 89.7371s
	iters: 800, epoch: 4 | loss: 0.2331471
	speed: 0.0359s/iter; left time: 86.9658s
Epoch: 4 cost time: 29.138769149780273
Epoch: 4, Steps: 806 | Train Loss: 0.2132794 Vali Loss: 0.3429099 Test Loss: 0.3650286
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : 2_5_24labellen_Autoformer_Energy_ftM_sl96_ll24_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 8617
test shape: (8617, 24, 16) (8617, 24, 16)
test shape: (8617, 24, 16) (8617, 24, 16)
model_id: 2_5_24labellen, mse:0.34597501158714294, mae:0.3895873427391052, rmse:0.5881963968276978 mape:2.6164376735687256 mspe:5762.7431640625
2025-02-11:22:59:04,515 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:22:59:04,516 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='2_5_72labellen', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=72, pred_len=24, bucket_size=4, n_hashes=4, enc_in=16, dec_in=16, c_out=16, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=3, val_yrs=1, test_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 2_5_72labellen_Autoformer_Energy_ftM_sl96_ll72_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 25801
val 8617
test 8617
	iters: 100, epoch: 1 | loss: 0.3831899
	speed: 0.0479s/iter; left time: 265.5575s
	iters: 200, epoch: 1 | loss: 0.3458639
	speed: 0.0414s/iter; left time: 225.2936s
	iters: 300, epoch: 1 | loss: 0.3047954
	speed: 0.0388s/iter; left time: 207.4009s
	iters: 400, epoch: 1 | loss: 0.3722609
	speed: 0.0388s/iter; left time: 203.4846s
	iters: 500, epoch: 1 | loss: 0.3000617
	speed: 0.0390s/iter; left time: 200.4876s
	iters: 600, epoch: 1 | loss: 0.3463647
	speed: 0.0390s/iter; left time: 196.6440s
	iters: 700, epoch: 1 | loss: 0.3349027
	speed: 0.0389s/iter; left time: 192.0384s
	iters: 800, epoch: 1 | loss: 0.2831278
	speed: 0.0388s/iter; left time: 188.1339s
Epoch: 1 cost time: 32.56095504760742
Epoch: 1, Steps: 806 | Train Loss: 0.3939880 Vali Loss: 0.3485031 Test Loss: 0.3667341
Validation loss decreased (inf --> 0.348503).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.2825391
	speed: 0.1712s/iter; left time: 811.1619s
	iters: 200, epoch: 2 | loss: 0.2914652
	speed: 0.0392s/iter; left time: 181.9446s
	iters: 300, epoch: 2 | loss: 0.3297860
	speed: 0.0392s/iter; left time: 178.0002s
	iters: 400, epoch: 2 | loss: 0.2681429
	speed: 0.0408s/iter; left time: 180.8387s
	iters: 500, epoch: 2 | loss: 0.2005216
	speed: 0.0412s/iter; left time: 178.8953s
	iters: 600, epoch: 2 | loss: 0.2961221
	speed: 0.0392s/iter; left time: 165.9490s
	iters: 700, epoch: 2 | loss: 0.2657948
	speed: 0.0389s/iter; left time: 160.9150s
	iters: 800, epoch: 2 | loss: 0.3387425
	speed: 0.0389s/iter; left time: 157.0858s
Epoch: 2 cost time: 32.33680868148804
Epoch: 2, Steps: 806 | Train Loss: 0.2904905 Vali Loss: 0.3454968 Test Loss: 0.3783041
Validation loss decreased (0.348503 --> 0.345497).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.2184141
	speed: 0.1700s/iter; left time: 668.4163s
	iters: 200, epoch: 3 | loss: 0.2428756
	speed: 0.0389s/iter; left time: 148.9820s
	iters: 300, epoch: 3 | loss: 0.2152894
	speed: 0.0418s/iter; left time: 155.9201s
	iters: 400, epoch: 3 | loss: 0.2153360
	speed: 0.0389s/iter; left time: 141.4209s
	iters: 500, epoch: 3 | loss: 0.2797179
	speed: 0.0389s/iter; left time: 137.5311s
	iters: 600, epoch: 3 | loss: 0.2213033
	speed: 0.0387s/iter; left time: 132.9467s
	iters: 700, epoch: 3 | loss: 0.2836393
	speed: 0.0388s/iter; left time: 129.1610s
	iters: 800, epoch: 3 | loss: 0.2602783
	speed: 0.0388s/iter; left time: 125.4776s
Epoch: 3 cost time: 32.02727150917053
Epoch: 3, Steps: 806 | Train Loss: 0.2370790 Vali Loss: 0.3433285 Test Loss: 0.3732767
Validation loss decreased (0.345497 --> 0.343329).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.1825675
	speed: 0.1736s/iter; left time: 542.6172s
	iters: 200, epoch: 4 | loss: 0.2547984
	speed: 0.0406s/iter; left time: 122.9586s
	iters: 300, epoch: 4 | loss: 0.1794028
	speed: 0.0402s/iter; left time: 117.5813s
	iters: 400, epoch: 4 | loss: 0.1938402
	speed: 0.0391s/iter; left time: 110.3247s
	iters: 500, epoch: 4 | loss: 0.1892876
	speed: 0.0391s/iter; left time: 106.6386s
	iters: 600, epoch: 4 | loss: 0.1917056
	speed: 0.0391s/iter; left time: 102.6302s
	iters: 700, epoch: 4 | loss: 0.1767675
	speed: 0.0395s/iter; left time: 99.7143s
	iters: 800, epoch: 4 | loss: 0.2149723
	speed: 0.0392s/iter; left time: 95.1462s
Epoch: 4 cost time: 32.49569797515869
Epoch: 4, Steps: 806 | Train Loss: 0.2109790 Vali Loss: 0.3448759 Test Loss: 0.3815241
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.1970364
	speed: 0.1697s/iter; left time: 393.5777s
	iters: 200, epoch: 5 | loss: 0.2037398
	speed: 0.0388s/iter; left time: 86.0631s
	iters: 300, epoch: 5 | loss: 0.2011356
	speed: 0.0387s/iter; left time: 82.0394s
	iters: 400, epoch: 5 | loss: 0.2275201
	speed: 0.0387s/iter; left time: 78.2358s
	iters: 500, epoch: 5 | loss: 0.2088228
	speed: 0.0396s/iter; left time: 75.9022s
	iters: 600, epoch: 5 | loss: 0.1614554
	speed: 0.0389s/iter; left time: 70.8325s
	iters: 700, epoch: 5 | loss: 0.2074396
	speed: 0.0390s/iter; left time: 67.0150s
	iters: 800, epoch: 5 | loss: 0.1844575
	speed: 0.0390s/iter; left time: 63.1185s
Epoch: 5 cost time: 31.751853704452515
Epoch: 5, Steps: 806 | Train Loss: 0.1978745 Vali Loss: 0.3497226 Test Loss: 0.3922693
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.1975982
	speed: 0.1750s/iter; left time: 264.8379s
	iters: 200, epoch: 6 | loss: 0.1968289
	speed: 0.0451s/iter; left time: 63.7958s
	iters: 300, epoch: 6 | loss: 0.1790985
	speed: 0.0441s/iter; left time: 57.8648s
	iters: 400, epoch: 6 | loss: 0.1704267
	speed: 0.0451s/iter; left time: 54.6596s
	iters: 500, epoch: 6 | loss: 0.1774994
	speed: 0.0443s/iter; left time: 49.3518s
	iters: 600, epoch: 6 | loss: 0.1759294
	speed: 0.0445s/iter; left time: 45.1148s
	iters: 700, epoch: 6 | loss: 0.1690708
	speed: 0.0440s/iter; left time: 40.1496s
	iters: 800, epoch: 6 | loss: 0.1832466
	speed: 0.0426s/iter; left time: 34.6211s
Epoch: 6 cost time: 36.1275315284729
Epoch: 6, Steps: 806 | Train Loss: 0.1911019 Vali Loss: 0.3519168 Test Loss: 0.3919925
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : 2_5_72labellen_Autoformer_Energy_ftM_sl96_ll72_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 8617
test shape: (8617, 24, 16) (8617, 24, 16)
test shape: (8617, 24, 16) (8617, 24, 16)
model_id: 2_5_72labellen, mse:0.3735986053943634, mae:0.407001256942749, rmse:0.6112271547317505 mape:2.719085454940796 mspe:7169.61181640625
2025-02-11:23:03:48,161 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:23:03:48,161 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='2_6_12predlen', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=12, bucket_size=4, n_hashes=4, enc_in=16, dec_in=16, c_out=16, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=3, val_yrs=1, test_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 2_6_12predlen_Autoformer_Energy_ftM_sl96_ll48_pl12_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 25813
val 8629
test 8629
	iters: 100, epoch: 1 | loss: 0.4754590
	speed: 0.0424s/iter; left time: 234.9640s
	iters: 200, epoch: 1 | loss: 0.3513837
	speed: 0.0363s/iter; left time: 197.3628s
	iters: 300, epoch: 1 | loss: 0.2911703
	speed: 0.0363s/iter; left time: 193.9551s
	iters: 400, epoch: 1 | loss: 0.2847457
	speed: 0.0350s/iter; left time: 183.4817s
	iters: 500, epoch: 1 | loss: 0.2612023
	speed: 0.0347s/iter; left time: 178.2280s
	iters: 600, epoch: 1 | loss: 0.2759481
	speed: 0.0355s/iter; left time: 178.8335s
	iters: 700, epoch: 1 | loss: 0.2673697
	speed: 0.0364s/iter; left time: 180.1579s
	iters: 800, epoch: 1 | loss: 0.3162169
	speed: 0.0354s/iter; left time: 171.2270s
Epoch: 1 cost time: 29.46274709701538
Epoch: 1, Steps: 806 | Train Loss: 0.3308084 Vali Loss: 0.2741347 Test Loss: 0.2855336
Validation loss decreased (inf --> 0.274135).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.2648678
	speed: 0.1541s/iter; left time: 729.9742s
	iters: 200, epoch: 2 | loss: 0.2308199
	speed: 0.0394s/iter; left time: 182.7356s
	iters: 300, epoch: 2 | loss: 0.2263660
	speed: 0.0358s/iter; left time: 162.5784s
	iters: 400, epoch: 2 | loss: 0.2869055
	speed: 0.0357s/iter; left time: 158.4959s
	iters: 500, epoch: 2 | loss: 0.2272964
	speed: 0.0357s/iter; left time: 154.6763s
	iters: 600, epoch: 2 | loss: 0.2135598
	speed: 0.0356s/iter; left time: 150.8976s
	iters: 700, epoch: 2 | loss: 0.2285740
	speed: 0.0357s/iter; left time: 147.5058s
	iters: 800, epoch: 2 | loss: 0.2620146
	speed: 0.0356s/iter; left time: 143.5172s
Epoch: 2 cost time: 29.899396181106567
Epoch: 2, Steps: 806 | Train Loss: 0.2428687 Vali Loss: 0.2648656 Test Loss: 0.2784724
Validation loss decreased (0.274135 --> 0.264866).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.2107401
	speed: 0.1506s/iter; left time: 592.1542s
	iters: 200, epoch: 3 | loss: 0.1990731
	speed: 0.0356s/iter; left time: 136.2288s
	iters: 300, epoch: 3 | loss: 0.2009540
	speed: 0.0358s/iter; left time: 133.6728s
	iters: 400, epoch: 3 | loss: 0.1682129
	speed: 0.0365s/iter; left time: 132.3824s
	iters: 500, epoch: 3 | loss: 0.2118794
	speed: 0.0363s/iter; left time: 128.1347s
	iters: 600, epoch: 3 | loss: 0.2135819
	speed: 0.0369s/iter; left time: 126.6656s
	iters: 700, epoch: 3 | loss: 0.1894218
	speed: 0.0379s/iter; left time: 126.3466s
	iters: 800, epoch: 3 | loss: 0.1738677
	speed: 0.0357s/iter; left time: 115.3256s
Epoch: 3 cost time: 29.68410301208496
Epoch: 3, Steps: 806 | Train Loss: 0.2038294 Vali Loss: 0.2497320 Test Loss: 0.2680624
Validation loss decreased (0.264866 --> 0.249732).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.1630799
	speed: 0.1510s/iter; left time: 471.9369s
	iters: 200, epoch: 4 | loss: 0.2187945
	speed: 0.0391s/iter; left time: 118.2544s
	iters: 300, epoch: 4 | loss: 0.1430731
	speed: 0.0366s/iter; left time: 106.9782s
	iters: 400, epoch: 4 | loss: 0.2114842
	speed: 0.0375s/iter; left time: 105.9230s
	iters: 500, epoch: 4 | loss: 0.1623109
	speed: 0.0391s/iter; left time: 106.5148s
	iters: 600, epoch: 4 | loss: 0.1758445
	speed: 0.0401s/iter; left time: 105.3322s
	iters: 700, epoch: 4 | loss: 0.2324183
	speed: 0.0397s/iter; left time: 100.1782s
	iters: 800, epoch: 4 | loss: 0.1714429
	speed: 0.0363s/iter; left time: 88.1478s
Epoch: 4 cost time: 31.09339189529419
Epoch: 4, Steps: 806 | Train Loss: 0.1831064 Vali Loss: 0.2562666 Test Loss: 0.2764345
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.2017066
	speed: 0.1491s/iter; left time: 345.6542s
	iters: 200, epoch: 5 | loss: 0.1721039
	speed: 0.0367s/iter; left time: 81.3300s
	iters: 300, epoch: 5 | loss: 0.1991156
	speed: 0.0364s/iter; left time: 77.1476s
	iters: 400, epoch: 5 | loss: 0.1595534
	speed: 0.0366s/iter; left time: 73.9690s
	iters: 500, epoch: 5 | loss: 0.1615673
	speed: 0.0366s/iter; left time: 70.1568s
	iters: 600, epoch: 5 | loss: 0.1482852
	speed: 0.0364s/iter; left time: 66.2914s
	iters: 700, epoch: 5 | loss: 0.1710978
	speed: 0.0364s/iter; left time: 62.5909s
	iters: 800, epoch: 5 | loss: 0.1792468
	speed: 0.0363s/iter; left time: 58.7975s
Epoch: 5 cost time: 29.73549199104309
Epoch: 5, Steps: 806 | Train Loss: 0.1723421 Vali Loss: 0.2560405 Test Loss: 0.2749129
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.2512892
	speed: 0.1506s/iter; left time: 227.8573s
	iters: 200, epoch: 6 | loss: 0.1630493
	speed: 0.0398s/iter; left time: 56.3069s
	iters: 300, epoch: 6 | loss: 0.2453928
	speed: 0.0364s/iter; left time: 47.7821s
	iters: 400, epoch: 6 | loss: 0.1447102
	speed: 0.0363s/iter; left time: 44.0553s
	iters: 500, epoch: 6 | loss: 0.2090390
	speed: 0.0364s/iter; left time: 40.5598s
	iters: 600, epoch: 6 | loss: 0.1577889
	speed: 0.0362s/iter; left time: 36.6542s
	iters: 700, epoch: 6 | loss: 0.1783311
	speed: 0.0365s/iter; left time: 33.2833s
	iters: 800, epoch: 6 | loss: 0.1440660
	speed: 0.0364s/iter; left time: 29.6256s
Epoch: 6 cost time: 30.119323015213013
Epoch: 6, Steps: 806 | Train Loss: 0.1663937 Vali Loss: 0.2589032 Test Loss: 0.2787659
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : 2_6_12predlen_Autoformer_Energy_ftM_sl96_ll48_pl12_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 8629
test shape: (8629, 12, 16) (8629, 12, 16)
test shape: (8629, 12, 16) (8629, 12, 16)
model_id: 2_6_12predlen, mse:0.26823484897613525, mae:0.34288570284843445, rmse:0.5179139375686646 mape:2.373863697052002 mspe:4386.54443359375
2025-02-11:23:08:03,398 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:23:08:03,398 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='2_6_48predlen', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=48, bucket_size=4, n_hashes=4, enc_in=16, dec_in=16, c_out=16, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=3, val_yrs=1, test_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 2_6_48predlen_Autoformer_Energy_ftM_sl96_ll48_pl48_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 25777
val 8593
test 8593
	iters: 100, epoch: 1 | loss: 0.4885141
	speed: 0.0449s/iter; left time: 248.4026s
	iters: 200, epoch: 1 | loss: 0.4857394
	speed: 0.0401s/iter; left time: 217.7655s
	iters: 300, epoch: 1 | loss: 0.5941547
	speed: 0.0415s/iter; left time: 221.5099s
	iters: 400, epoch: 1 | loss: 0.4816930
	speed: 0.0390s/iter; left time: 204.2929s
	iters: 500, epoch: 1 | loss: 0.4090440
	speed: 0.0390s/iter; left time: 200.5078s
	iters: 600, epoch: 1 | loss: 0.3820747
	speed: 0.0391s/iter; left time: 196.7066s
	iters: 700, epoch: 1 | loss: 0.3978719
	speed: 0.0390s/iter; left time: 192.5649s
	iters: 800, epoch: 1 | loss: 0.4069152
	speed: 0.0391s/iter; left time: 188.9532s
Epoch: 1 cost time: 32.441030740737915
Epoch: 1, Steps: 805 | Train Loss: 0.4506979 Vali Loss: 0.4603074 Test Loss: 0.4673402
Validation loss decreased (inf --> 0.460307).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.5026037
	speed: 0.1703s/iter; left time: 805.6001s
	iters: 200, epoch: 2 | loss: 0.3507719
	speed: 0.0394s/iter; left time: 182.4226s
	iters: 300, epoch: 2 | loss: 0.2769946
	speed: 0.0395s/iter; left time: 179.1709s
	iters: 400, epoch: 2 | loss: 0.3112546
	speed: 0.0397s/iter; left time: 175.8301s
	iters: 500, epoch: 2 | loss: 0.3504952
	speed: 0.0390s/iter; left time: 168.7258s
	iters: 600, epoch: 2 | loss: 0.3560647
	speed: 0.0389s/iter; left time: 164.5023s
	iters: 700, epoch: 2 | loss: 0.3095444
	speed: 0.0391s/iter; left time: 161.6523s
	iters: 800, epoch: 2 | loss: 0.3694937
	speed: 0.0390s/iter; left time: 157.3696s
Epoch: 2 cost time: 31.984267473220825
Epoch: 2, Steps: 805 | Train Loss: 0.3378876 Vali Loss: 0.4763024 Test Loss: 0.4970832
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.2647702
	speed: 0.1727s/iter; left time: 678.0086s
	iters: 200, epoch: 3 | loss: 0.2962814
	speed: 0.0391s/iter; left time: 149.4430s
	iters: 300, epoch: 3 | loss: 0.2568394
	speed: 0.0391s/iter; left time: 145.5438s
	iters: 400, epoch: 3 | loss: 0.2910824
	speed: 0.0392s/iter; left time: 142.0263s
	iters: 500, epoch: 3 | loss: 0.2712656
	speed: 0.0399s/iter; left time: 140.8145s
	iters: 600, epoch: 3 | loss: 0.2569465
	speed: 0.0397s/iter; left time: 136.0836s
	iters: 700, epoch: 3 | loss: 0.2819788
	speed: 0.0405s/iter; left time: 134.5890s
	iters: 800, epoch: 3 | loss: 0.2415430
	speed: 0.0398s/iter; left time: 128.3999s
Epoch: 3 cost time: 32.436156272888184
Epoch: 3, Steps: 805 | Train Loss: 0.2692320 Vali Loss: 0.5120147 Test Loss: 0.5165969
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.2780626
	speed: 0.1708s/iter; left time: 533.1653s
	iters: 200, epoch: 4 | loss: 0.2249181
	speed: 0.0391s/iter; left time: 118.1780s
	iters: 300, epoch: 4 | loss: 0.2207785
	speed: 0.0392s/iter; left time: 114.3885s
	iters: 400, epoch: 4 | loss: 0.2977211
	speed: 0.0390s/iter; left time: 110.1130s
	iters: 500, epoch: 4 | loss: 0.2263521
	speed: 0.0391s/iter; left time: 106.2756s
	iters: 600, epoch: 4 | loss: 0.2452827
	speed: 0.0391s/iter; left time: 102.4401s
	iters: 700, epoch: 4 | loss: 0.2321632
	speed: 0.0391s/iter; left time: 98.4785s
	iters: 800, epoch: 4 | loss: 0.2278641
	speed: 0.0390s/iter; left time: 94.5191s
Epoch: 4 cost time: 31.84504246711731
Epoch: 4, Steps: 805 | Train Loss: 0.2361724 Vali Loss: 0.5282647 Test Loss: 0.5320079
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : 2_6_48predlen_Autoformer_Energy_ftM_sl96_ll48_pl48_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 8593
test shape: (8593, 48, 16) (8593, 48, 16)
test shape: (8593, 48, 16) (8593, 48, 16)
model_id: 2_6_48predlen, mse:0.46759095788002014, mae:0.4575566053390503, rmse:0.6838062405586243 mape:2.7623519897460938 mspe:4657.267578125
2025-02-11:23:11:13,822 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:23:11:13,822 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='2_6_72predlen', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=72, bucket_size=4, n_hashes=4, enc_in=16, dec_in=16, c_out=16, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=3, val_yrs=1, test_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 2_6_72predlen_Autoformer_Energy_ftM_sl96_ll48_pl72_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 25753
val 8569
test 8569
	iters: 100, epoch: 1 | loss: 0.4618616
	speed: 0.0480s/iter; left time: 265.1954s
	iters: 200, epoch: 1 | loss: 0.5737492
	speed: 0.0419s/iter; left time: 227.6220s
	iters: 300, epoch: 1 | loss: 0.4673308
	speed: 0.0419s/iter; left time: 223.2873s
	iters: 400, epoch: 1 | loss: 0.4921395
	speed: 0.0421s/iter; left time: 220.1101s
	iters: 500, epoch: 1 | loss: 0.4410354
	speed: 0.0421s/iter; left time: 215.6886s
	iters: 600, epoch: 1 | loss: 0.4508503
	speed: 0.0429s/iter; left time: 215.5363s
	iters: 700, epoch: 1 | loss: 0.4146140
	speed: 0.0427s/iter; left time: 210.5358s
	iters: 800, epoch: 1 | loss: 0.5338038
	speed: 0.0426s/iter; left time: 205.6520s
Epoch: 1 cost time: 34.64295244216919
Epoch: 1, Steps: 804 | Train Loss: 0.4807406 Vali Loss: 0.4882073 Test Loss: 0.5197458
Validation loss decreased (inf --> 0.488207).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.3589420
	speed: 0.1895s/iter; left time: 895.3980s
	iters: 200, epoch: 2 | loss: 0.3312193
	speed: 0.0436s/iter; left time: 201.5838s
	iters: 300, epoch: 2 | loss: 0.4052047
	speed: 0.0429s/iter; left time: 194.3354s
	iters: 400, epoch: 2 | loss: 0.3635536
	speed: 0.0422s/iter; left time: 186.5157s
	iters: 500, epoch: 2 | loss: 0.3736525
	speed: 0.0420s/iter; left time: 181.5738s
	iters: 600, epoch: 2 | loss: 0.2951219
	speed: 0.0420s/iter; left time: 177.3626s
	iters: 700, epoch: 2 | loss: 0.3188716
	speed: 0.0421s/iter; left time: 173.4645s
	iters: 800, epoch: 2 | loss: 0.2910878
	speed: 0.0421s/iter; left time: 169.3390s
Epoch: 2 cost time: 34.80632472038269
Epoch: 2, Steps: 804 | Train Loss: 0.3550675 Vali Loss: 0.5477796 Test Loss: 0.5805944
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.2846736
	speed: 0.1868s/iter; left time: 732.6110s
	iters: 200, epoch: 3 | loss: 0.2668703
	speed: 0.0422s/iter; left time: 161.2129s
	iters: 300, epoch: 3 | loss: 0.2966279
	speed: 0.0423s/iter; left time: 157.3074s
	iters: 400, epoch: 3 | loss: 0.2957884
	speed: 0.0430s/iter; left time: 155.8483s
	iters: 500, epoch: 3 | loss: 0.2670476
	speed: 0.0422s/iter; left time: 148.6172s
	iters: 600, epoch: 3 | loss: 0.2346325
	speed: 0.0422s/iter; left time: 144.2014s
	iters: 700, epoch: 3 | loss: 0.2903245
	speed: 0.0421s/iter; left time: 139.9800s
	iters: 800, epoch: 3 | loss: 0.2536111
	speed: 0.0430s/iter; left time: 138.3636s
Epoch: 3 cost time: 34.45443511009216
Epoch: 3, Steps: 804 | Train Loss: 0.2745556 Vali Loss: 0.5720474 Test Loss: 0.6014510
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.2275283
	speed: 0.1868s/iter; left time: 582.4032s
	iters: 200, epoch: 4 | loss: 0.2503993
	speed: 0.0421s/iter; left time: 127.1023s
	iters: 300, epoch: 4 | loss: 0.2259446
	speed: 0.0423s/iter; left time: 123.4407s
	iters: 400, epoch: 4 | loss: 0.2377214
	speed: 0.0422s/iter; left time: 118.8480s
	iters: 500, epoch: 4 | loss: 0.2435690
	speed: 0.0421s/iter; left time: 114.3522s
	iters: 600, epoch: 4 | loss: 0.2257183
	speed: 0.0421s/iter; left time: 110.2216s
	iters: 700, epoch: 4 | loss: 0.2136181
	speed: 0.0423s/iter; left time: 106.4101s
	iters: 800, epoch: 4 | loss: 0.2177179
	speed: 0.0422s/iter; left time: 102.0410s
Epoch: 4 cost time: 34.29366707801819
Epoch: 4, Steps: 804 | Train Loss: 0.2397632 Vali Loss: 0.6188463 Test Loss: 0.6280918
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : 2_6_72predlen_Autoformer_Energy_ftM_sl96_ll48_pl72_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 8569
test shape: (8569, 72, 16) (8569, 72, 16)
test shape: (8569, 72, 16) (8569, 72, 16)
model_id: 2_6_72predlen, mse:0.5198439955711365, mae:0.49620068073272705, rmse:0.7210021018981934 mape:3.038418769836426 mspe:5566.7802734375
2025-02-11:23:14:39,813 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:23:14:39,813 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='2_7_1encoder', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=24, bucket_size=4, n_hashes=4, enc_in=16, dec_in=16, c_out=16, d_model=512, n_heads=8, e_layers=1, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=3, val_yrs=1, test_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 2_7_1encoder_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el1_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 25801
val 8617
test 8617
	iters: 100, epoch: 1 | loss: 0.5645586
	speed: 0.0347s/iter; left time: 192.4299s
	iters: 200, epoch: 1 | loss: 0.3860082
	speed: 0.0288s/iter; left time: 156.7888s
	iters: 300, epoch: 1 | loss: 0.3434788
	speed: 0.0274s/iter; left time: 146.5995s
	iters: 400, epoch: 1 | loss: 0.5031062
	speed: 0.0341s/iter; left time: 178.6260s
	iters: 500, epoch: 1 | loss: 0.3836812
	speed: 0.0322s/iter; left time: 165.8075s
	iters: 600, epoch: 1 | loss: 0.3453408
	speed: 0.0273s/iter; left time: 137.7197s
	iters: 700, epoch: 1 | loss: 0.2782905
	speed: 0.0269s/iter; left time: 132.8547s
	iters: 800, epoch: 1 | loss: 0.3286568
	speed: 0.0266s/iter; left time: 128.9172s
Epoch: 1 cost time: 24.02030301094055
Epoch: 1, Steps: 806 | Train Loss: 0.3967959 Vali Loss: 0.3491096 Test Loss: 0.3672387
Validation loss decreased (inf --> 0.349110).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.3741851
	speed: 0.1268s/iter; left time: 600.5940s
	iters: 200, epoch: 2 | loss: 0.2844688
	speed: 0.0333s/iter; left time: 154.5961s
	iters: 300, epoch: 2 | loss: 0.3246505
	speed: 0.0270s/iter; left time: 122.6794s
	iters: 400, epoch: 2 | loss: 0.3464998
	speed: 0.0316s/iter; left time: 140.4249s
	iters: 500, epoch: 2 | loss: 0.2937962
	speed: 0.0275s/iter; left time: 119.2504s
	iters: 600, epoch: 2 | loss: 0.2702097
	speed: 0.0273s/iter; left time: 115.7485s
	iters: 700, epoch: 2 | loss: 0.3533123
	speed: 0.0319s/iter; left time: 132.0275s
	iters: 800, epoch: 2 | loss: 0.2659067
	speed: 0.0269s/iter; left time: 108.5496s
Epoch: 2 cost time: 24.539559364318848
Epoch: 2, Steps: 806 | Train Loss: 0.3057096 Vali Loss: 0.3562279 Test Loss: 0.3744182
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.2432733
	speed: 0.1303s/iter; left time: 512.3187s
	iters: 200, epoch: 3 | loss: 0.2351575
	speed: 0.0328s/iter; left time: 125.6840s
	iters: 300, epoch: 3 | loss: 0.2882805
	speed: 0.0275s/iter; left time: 102.4511s
	iters: 400, epoch: 3 | loss: 0.2431245
	speed: 0.0292s/iter; left time: 105.9410s
	iters: 500, epoch: 3 | loss: 0.2463476
	speed: 0.0274s/iter; left time: 96.8028s
	iters: 600, epoch: 3 | loss: 0.3267112
	speed: 0.0274s/iter; left time: 93.8450s
	iters: 700, epoch: 3 | loss: 0.2198921
	speed: 0.0274s/iter; left time: 91.3196s
	iters: 800, epoch: 3 | loss: 0.3215585
	speed: 0.0359s/iter; left time: 116.0086s
Epoch: 3 cost time: 25.158653259277344
Epoch: 3, Steps: 806 | Train Loss: 0.2601326 Vali Loss: 0.3625651 Test Loss: 0.3838771
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.3163753
	speed: 0.1283s/iter; left time: 400.8961s
	iters: 200, epoch: 4 | loss: 0.2173031
	speed: 0.0329s/iter; left time: 99.6139s
	iters: 300, epoch: 4 | loss: 0.2913544
	speed: 0.0356s/iter; left time: 104.2724s
	iters: 400, epoch: 4 | loss: 0.2415426
	speed: 0.0339s/iter; left time: 95.7671s
	iters: 500, epoch: 4 | loss: 0.2362816
	speed: 0.0349s/iter; left time: 95.1602s
	iters: 600, epoch: 4 | loss: 0.2012049
	speed: 0.0335s/iter; left time: 87.9674s
	iters: 700, epoch: 4 | loss: 0.2402510
	speed: 0.0310s/iter; left time: 78.3082s
	iters: 800, epoch: 4 | loss: 0.2062896
	speed: 0.0281s/iter; left time: 68.0954s
Epoch: 4 cost time: 27.059284687042236
Epoch: 4, Steps: 806 | Train Loss: 0.2377336 Vali Loss: 0.3597730 Test Loss: 0.3799163
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : 2_7_1encoder_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el1_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 8617
test shape: (8617, 24, 16) (8617, 24, 16)
test shape: (8617, 24, 16) (8617, 24, 16)
model_id: 2_7_1encoder, mse:0.36763522028923035, mae:0.40868112444877625, rmse:0.6063293218612671 mape:2.613875150680542 mspe:5524.41259765625
2025-02-11:23:17:04,891 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:23:17:04,891 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='2_7_3encoders', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=24, bucket_size=4, n_hashes=4, enc_in=16, dec_in=16, c_out=16, d_model=512, n_heads=8, e_layers=3, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=3, val_yrs=1, test_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 2_7_3encoders_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el3_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 25801
val 8617
test 8617
	iters: 100, epoch: 1 | loss: 0.5049290
	speed: 0.0543s/iter; left time: 300.8883s
	iters: 200, epoch: 1 | loss: 0.4031542
	speed: 0.0478s/iter; left time: 260.3019s
	iters: 300, epoch: 1 | loss: 0.3235627
	speed: 0.0474s/iter; left time: 253.4308s
	iters: 400, epoch: 1 | loss: 0.3761140
	speed: 0.0466s/iter; left time: 244.1950s
	iters: 500, epoch: 1 | loss: 0.3894493
	speed: 0.0465s/iter; left time: 239.1781s
	iters: 600, epoch: 1 | loss: 0.3350182
	speed: 0.0469s/iter; left time: 236.4429s
	iters: 700, epoch: 1 | loss: 0.2403780
	speed: 0.0478s/iter; left time: 236.2371s
	iters: 800, epoch: 1 | loss: 0.2461283
	speed: 0.0467s/iter; left time: 226.0225s
Epoch: 1 cost time: 38.74357628822327
Epoch: 1, Steps: 806 | Train Loss: 0.3900502 Vali Loss: 0.3983158 Test Loss: 0.4193792
Validation loss decreased (inf --> 0.398316).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.2688264
	speed: 0.1911s/iter; left time: 905.1704s
	iters: 200, epoch: 2 | loss: 0.2990955
	speed: 0.0493s/iter; left time: 228.5499s
	iters: 300, epoch: 2 | loss: 0.2941977
	speed: 0.0487s/iter; left time: 220.9152s
	iters: 400, epoch: 2 | loss: 0.3029633
	speed: 0.0456s/iter; left time: 202.3668s
	iters: 500, epoch: 2 | loss: 0.3002005
	speed: 0.0460s/iter; left time: 199.4234s
	iters: 600, epoch: 2 | loss: 0.2564867
	speed: 0.0459s/iter; left time: 194.4266s
	iters: 700, epoch: 2 | loss: 0.2514782
	speed: 0.0466s/iter; left time: 192.6543s
	iters: 800, epoch: 2 | loss: 0.2167291
	speed: 0.0465s/iter; left time: 187.6545s
Epoch: 2 cost time: 38.15241265296936
Epoch: 2, Steps: 806 | Train Loss: 0.2889069 Vali Loss: 0.3745888 Test Loss: 0.3966872
Validation loss decreased (0.398316 --> 0.374589).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.2274350
	speed: 0.1933s/iter; left time: 759.9171s
	iters: 200, epoch: 3 | loss: 0.2299140
	speed: 0.0471s/iter; left time: 180.3084s
	iters: 300, epoch: 3 | loss: 0.2273252
	speed: 0.0480s/iter; left time: 179.1766s
	iters: 400, epoch: 3 | loss: 0.2238344
	speed: 0.0464s/iter; left time: 168.5012s
	iters: 500, epoch: 3 | loss: 0.2227579
	speed: 0.0472s/iter; left time: 166.5478s
	iters: 600, epoch: 3 | loss: 0.2535607
	speed: 0.0465s/iter; left time: 159.6157s
	iters: 700, epoch: 3 | loss: 0.1919537
	speed: 0.0472s/iter; left time: 157.2999s
	iters: 800, epoch: 3 | loss: 0.1987488
	speed: 0.0467s/iter; left time: 150.8826s
Epoch: 3 cost time: 38.32343339920044
Epoch: 3, Steps: 806 | Train Loss: 0.2252019 Vali Loss: 0.3686387 Test Loss: 0.3987893
Validation loss decreased (0.374589 --> 0.368639).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.1994178
	speed: 0.1933s/iter; left time: 603.9499s
	iters: 200, epoch: 4 | loss: 0.2103571
	speed: 0.0463s/iter; left time: 140.0253s
	iters: 300, epoch: 4 | loss: 0.1878785
	speed: 0.0465s/iter; left time: 135.9103s
	iters: 400, epoch: 4 | loss: 0.2205434
	speed: 0.0464s/iter; left time: 131.1777s
	iters: 500, epoch: 4 | loss: 0.2332079
	speed: 0.0464s/iter; left time: 126.4828s
	iters: 600, epoch: 4 | loss: 0.1711176
	speed: 0.0465s/iter; left time: 121.9368s
	iters: 700, epoch: 4 | loss: 0.2344982
	speed: 0.0471s/iter; left time: 118.8679s
	iters: 800, epoch: 4 | loss: 0.1727916
	speed: 0.0464s/iter; left time: 112.5504s
Epoch: 4 cost time: 37.93819761276245
Epoch: 4, Steps: 806 | Train Loss: 0.1946515 Vali Loss: 0.3695661 Test Loss: 0.3976200
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.1893122
	speed: 0.1903s/iter; left time: 441.3589s
	iters: 200, epoch: 5 | loss: 0.1680443
	speed: 0.0466s/iter; left time: 103.5107s
	iters: 300, epoch: 5 | loss: 0.1653929
	speed: 0.0473s/iter; left time: 100.3104s
	iters: 400, epoch: 5 | loss: 0.1716190
	speed: 0.0464s/iter; left time: 93.7030s
	iters: 500, epoch: 5 | loss: 0.1707010
	speed: 0.0467s/iter; left time: 89.5224s
	iters: 600, epoch: 5 | loss: 0.1683970
	speed: 0.0466s/iter; left time: 84.6880s
	iters: 700, epoch: 5 | loss: 0.1735957
	speed: 0.0466s/iter; left time: 80.0799s
	iters: 800, epoch: 5 | loss: 0.1789682
	speed: 0.0468s/iter; left time: 75.8061s
Epoch: 5 cost time: 38.00096774101257
Epoch: 5, Steps: 806 | Train Loss: 0.1802891 Vali Loss: 0.3732852 Test Loss: 0.4002765
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.1601997
	speed: 0.1911s/iter; left time: 289.0823s
	iters: 200, epoch: 6 | loss: 0.1723168
	speed: 0.0474s/iter; left time: 66.9411s
	iters: 300, epoch: 6 | loss: 0.1598323
	speed: 0.0468s/iter; left time: 61.4933s
	iters: 400, epoch: 6 | loss: 0.1747749
	speed: 0.0484s/iter; left time: 58.7446s
	iters: 500, epoch: 6 | loss: 0.1829756
	speed: 0.0472s/iter; left time: 52.5488s
	iters: 600, epoch: 6 | loss: 0.1687657
	speed: 0.0469s/iter; left time: 47.5256s
	iters: 700, epoch: 6 | loss: 0.1876862
	speed: 0.0469s/iter; left time: 42.7805s
	iters: 800, epoch: 6 | loss: 0.1231537
	speed: 0.0472s/iter; left time: 38.3847s
Epoch: 6 cost time: 38.47254800796509
Epoch: 6, Steps: 806 | Train Loss: 0.1732684 Vali Loss: 0.3747668 Test Loss: 0.4017456
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : 2_7_3encoders_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el3_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 8617
test shape: (8617, 24, 16) (8617, 24, 16)
test shape: (8617, 24, 16) (8617, 24, 16)
model_id: 2_7_3encoders, mse:0.3991972506046295, mae:0.4211578369140625, rmse:0.631820559501648 mape:2.742648124694824 mspe:5023.84521484375
2025-02-11:23:22:29,574 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:23:22:29,574 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='2_8_2decoders', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=24, bucket_size=4, n_hashes=4, enc_in=16, dec_in=16, c_out=16, d_model=512, n_heads=8, e_layers=2, d_layers=2, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=3, val_yrs=1, test_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 2_8_2decoders_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 25801
val 8617
test 8617
	iters: 100, epoch: 1 | loss: 0.4051499
	speed: 0.0569s/iter; left time: 315.3723s
	iters: 200, epoch: 1 | loss: 0.5012887
	speed: 0.0505s/iter; left time: 275.0206s
	iters: 300, epoch: 1 | loss: 0.3355733
	speed: 0.0504s/iter; left time: 269.3464s
	iters: 400, epoch: 1 | loss: 0.3824262
	speed: 0.0504s/iter; left time: 264.0783s
	iters: 500, epoch: 1 | loss: 0.3049711
	speed: 0.0505s/iter; left time: 259.6122s
	iters: 600, epoch: 1 | loss: 0.3033890
	speed: 0.0500s/iter; left time: 252.2670s
	iters: 700, epoch: 1 | loss: 0.3042369
	speed: 0.0505s/iter; left time: 249.5319s
	iters: 800, epoch: 1 | loss: 0.3156782
	speed: 0.0512s/iter; left time: 247.7679s
Epoch: 1 cost time: 41.420438289642334
Epoch: 1, Steps: 806 | Train Loss: 0.3538190 Vali Loss: 0.3551174 Test Loss: 0.3714878
Validation loss decreased (inf --> 0.355117).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.3137510
	speed: 0.2049s/iter; left time: 970.6861s
	iters: 200, epoch: 2 | loss: 0.2320196
	speed: 0.0516s/iter; left time: 239.1557s
	iters: 300, epoch: 2 | loss: 0.2478797
	speed: 0.0515s/iter; left time: 233.4993s
	iters: 400, epoch: 2 | loss: 0.3227772
	speed: 0.0517s/iter; left time: 229.3805s
	iters: 500, epoch: 2 | loss: 0.2083855
	speed: 0.0513s/iter; left time: 222.4620s
	iters: 600, epoch: 2 | loss: 0.2572811
	speed: 0.0512s/iter; left time: 216.9268s
	iters: 700, epoch: 2 | loss: 0.2165558
	speed: 0.0514s/iter; left time: 212.6216s
	iters: 800, epoch: 2 | loss: 0.1989745
	speed: 0.0503s/iter; left time: 203.2432s
Epoch: 2 cost time: 41.6598744392395
Epoch: 2, Steps: 806 | Train Loss: 0.2489312 Vali Loss: 0.3638253 Test Loss: 0.4027373
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.1889042
	speed: 0.2034s/iter; left time: 799.5025s
	iters: 200, epoch: 3 | loss: 0.1747707
	speed: 0.0514s/iter; left time: 196.9088s
	iters: 300, epoch: 3 | loss: 0.1987817
	speed: 0.0505s/iter; left time: 188.5246s
	iters: 400, epoch: 3 | loss: 0.1560567
	speed: 0.0508s/iter; left time: 184.6067s
	iters: 500, epoch: 3 | loss: 0.1940627
	speed: 0.0506s/iter; left time: 178.6823s
	iters: 600, epoch: 3 | loss: 0.1654259
	speed: 0.0511s/iter; left time: 175.4743s
	iters: 700, epoch: 3 | loss: 0.1991339
	speed: 0.0517s/iter; left time: 172.1620s
	iters: 800, epoch: 3 | loss: 0.1557591
	speed: 0.0505s/iter; left time: 163.3113s
Epoch: 3 cost time: 41.43160629272461
Epoch: 3, Steps: 806 | Train Loss: 0.1826356 Vali Loss: 0.3865041 Test Loss: 0.4197378
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.1555070
	speed: 0.2051s/iter; left time: 641.0030s
	iters: 200, epoch: 4 | loss: 0.1630619
	speed: 0.0509s/iter; left time: 154.0032s
	iters: 300, epoch: 4 | loss: 0.1471585
	speed: 0.0508s/iter; left time: 148.4832s
	iters: 400, epoch: 4 | loss: 0.1314227
	speed: 0.0510s/iter; left time: 144.0335s
	iters: 500, epoch: 4 | loss: 0.1688192
	speed: 0.0504s/iter; left time: 137.4330s
	iters: 600, epoch: 4 | loss: 0.1444075
	speed: 0.0516s/iter; left time: 135.3346s
	iters: 700, epoch: 4 | loss: 0.1508045
	speed: 0.0515s/iter; left time: 130.1361s
	iters: 800, epoch: 4 | loss: 0.1641894
	speed: 0.0510s/iter; left time: 123.6975s
Epoch: 4 cost time: 41.575628995895386
Epoch: 4, Steps: 806 | Train Loss: 0.1523160 Vali Loss: 0.3797557 Test Loss: 0.4158724
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : 2_8_2decoders_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 8617
test shape: (8617, 24, 16) (8617, 24, 16)
test shape: (8617, 24, 16) (8617, 24, 16)
model_id: 2_8_2decoders, mse:0.37197771668434143, mae:0.4054190218448639, rmse:0.6098997592926025 mape:2.733468770980835 mspe:7234.11376953125
2025-02-11:23:26:26,816 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:23:26:26,816 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='2_8_3decoders', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=24, bucket_size=4, n_hashes=4, enc_in=16, dec_in=16, c_out=16, d_model=512, n_heads=8, e_layers=2, d_layers=3, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=3, val_yrs=1, test_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 2_8_3decoders_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl3_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 25801
val 8617
test 8617
	iters: 100, epoch: 1 | loss: 0.3861250
	speed: 0.0709s/iter; left time: 393.0501s
	iters: 200, epoch: 1 | loss: 0.4764130
	speed: 0.0650s/iter; left time: 353.7839s
	iters: 300, epoch: 1 | loss: 0.4115817
	speed: 0.0644s/iter; left time: 344.0556s
	iters: 400, epoch: 1 | loss: 0.3041599
	speed: 0.0646s/iter; left time: 338.6446s
	iters: 500, epoch: 1 | loss: 0.2802972
	speed: 0.0642s/iter; left time: 330.0727s
	iters: 600, epoch: 1 | loss: 0.2562969
	speed: 0.0641s/iter; left time: 323.3850s
	iters: 700, epoch: 1 | loss: 0.2658035
	speed: 0.0642s/iter; left time: 317.1898s
	iters: 800, epoch: 1 | loss: 0.2021813
	speed: 0.0655s/iter; left time: 317.0687s
Epoch: 1 cost time: 52.77176570892334
Epoch: 1, Steps: 806 | Train Loss: 0.3429799 Vali Loss: 0.3646607 Test Loss: 0.3907909
Validation loss decreased (inf --> 0.364661).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.2066647
	speed: 0.2549s/iter; left time: 1207.3327s
	iters: 200, epoch: 2 | loss: 0.2519628
	speed: 0.0633s/iter; left time: 293.4799s
	iters: 300, epoch: 2 | loss: 0.2352221
	speed: 0.0632s/iter; left time: 286.6670s
	iters: 400, epoch: 2 | loss: 0.1842993
	speed: 0.0648s/iter; left time: 287.6237s
	iters: 500, epoch: 2 | loss: 0.2020323
	speed: 0.0637s/iter; left time: 276.3562s
	iters: 600, epoch: 2 | loss: 0.1512058
	speed: 0.0643s/iter; left time: 272.4998s
	iters: 700, epoch: 2 | loss: 0.1764412
	speed: 0.0645s/iter; left time: 266.9893s
	iters: 800, epoch: 2 | loss: 0.1867100
	speed: 0.0638s/iter; left time: 257.4000s
Epoch: 2 cost time: 51.96955227851868
Epoch: 2, Steps: 806 | Train Loss: 0.2139402 Vali Loss: 0.3920439 Test Loss: 0.4176351
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.1741375
	speed: 0.2510s/iter; left time: 986.5516s
	iters: 200, epoch: 3 | loss: 0.1315021
	speed: 0.0639s/iter; left time: 244.6710s
	iters: 300, epoch: 3 | loss: 0.1415362
	speed: 0.0639s/iter; left time: 238.4011s
	iters: 400, epoch: 3 | loss: 0.1466378
	speed: 0.0655s/iter; left time: 237.7660s
	iters: 500, epoch: 3 | loss: 0.1577739
	speed: 0.0649s/iter; left time: 229.1475s
	iters: 600, epoch: 3 | loss: 0.1356524
	speed: 0.0641s/iter; left time: 219.7915s
	iters: 700, epoch: 3 | loss: 0.1301261
	speed: 0.0646s/iter; left time: 215.1554s
	iters: 800, epoch: 3 | loss: 0.1197832
	speed: 0.0630s/iter; left time: 203.4719s
Epoch: 3 cost time: 51.908427000045776
Epoch: 3, Steps: 806 | Train Loss: 0.1436954 Vali Loss: 0.3966596 Test Loss: 0.4314005
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.1258873
	speed: 0.2516s/iter; left time: 786.2072s
	iters: 200, epoch: 4 | loss: 0.1182287
	speed: 0.0650s/iter; left time: 196.5228s
	iters: 300, epoch: 4 | loss: 0.1254084
	speed: 0.0651s/iter; left time: 190.4799s
	iters: 400, epoch: 4 | loss: 0.1101014
	speed: 0.0655s/iter; left time: 184.9288s
	iters: 500, epoch: 4 | loss: 0.1128377
	speed: 0.0650s/iter; left time: 177.0879s
	iters: 600, epoch: 4 | loss: 0.1052074
	speed: 0.0643s/iter; left time: 168.8901s
	iters: 700, epoch: 4 | loss: 0.1198062
	speed: 0.0642s/iter; left time: 162.1242s
	iters: 800, epoch: 4 | loss: 0.1016661
	speed: 0.0649s/iter; left time: 157.4799s
Epoch: 4 cost time: 52.589672565460205
Epoch: 4, Steps: 806 | Train Loss: 0.1177047 Vali Loss: 0.3877394 Test Loss: 0.4227926
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : 2_8_3decoders_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl3_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 8617
test shape: (8617, 24, 16) (8617, 24, 16)
test shape: (8617, 24, 16) (8617, 24, 16)
model_id: 2_8_3decoders, mse:0.3912592828273773, mae:0.42380237579345703, rmse:0.6255072355270386 mape:2.90287446975708 mspe:10369.05859375
2025-02-11:23:31:22,845 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:23:31:22,845 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='3_2train_1val_2test', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=24, bucket_size=4, n_hashes=4, enc_in=16, dec_in=16, c_out=16, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=2, val_yrs=1, test_yrs=2)
Use GPU: cuda:0
>>>>>>>start training : 3_2train_1val_2test_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17161
val 8617
test 17257
	iters: 100, epoch: 1 | loss: 0.3595130
	speed: 0.0451s/iter; left time: 164.6292s
	iters: 200, epoch: 1 | loss: 0.5043213
	speed: 0.0389s/iter; left time: 138.3786s
	iters: 300, epoch: 1 | loss: 0.3510672
	speed: 0.0407s/iter; left time: 140.7007s
	iters: 400, epoch: 1 | loss: 0.5817327
	speed: 0.0368s/iter; left time: 123.3203s
	iters: 500, epoch: 1 | loss: 0.4369839
	speed: 0.0359s/iter; left time: 116.7508s
Epoch: 1 cost time: 21.333521127700806
Epoch: 1, Steps: 536 | Train Loss: 0.4005496 Vali Loss: 0.3744279 Test Loss: 0.4162281
Validation loss decreased (inf --> 0.374428).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.3043717
	speed: 0.2217s/iter; left time: 690.9929s
	iters: 200, epoch: 2 | loss: 0.3129342
	speed: 0.0362s/iter; left time: 109.2839s
	iters: 300, epoch: 2 | loss: 0.2553658
	speed: 0.0382s/iter; left time: 111.3003s
	iters: 400, epoch: 2 | loss: 0.2804181
	speed: 0.0394s/iter; left time: 110.9857s
	iters: 500, epoch: 2 | loss: 0.2493806
	speed: 0.0368s/iter; left time: 99.8704s
Epoch: 2 cost time: 20.541468143463135
Epoch: 2, Steps: 536 | Train Loss: 0.2965483 Vali Loss: 0.3767879 Test Loss: 0.4412197
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.2790043
	speed: 0.2185s/iter; left time: 563.9141s
	iters: 200, epoch: 3 | loss: 0.2300973
	speed: 0.0364s/iter; left time: 90.3235s
	iters: 300, epoch: 3 | loss: 0.2149819
	speed: 0.0365s/iter; left time: 86.9264s
	iters: 400, epoch: 3 | loss: 0.2136322
	speed: 0.0367s/iter; left time: 83.7554s
	iters: 500, epoch: 3 | loss: 0.2225867
	speed: 0.0403s/iter; left time: 87.8323s
Epoch: 3 cost time: 20.436578035354614
Epoch: 3, Steps: 536 | Train Loss: 0.2428885 Vali Loss: 0.3911826 Test Loss: 0.4485525
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.1951236
	speed: 0.2202s/iter; left time: 450.2159s
	iters: 200, epoch: 4 | loss: 0.2174528
	speed: 0.0364s/iter; left time: 70.8279s
	iters: 300, epoch: 4 | loss: 0.1871733
	speed: 0.0393s/iter; left time: 72.5791s
	iters: 400, epoch: 4 | loss: 0.1765978
	speed: 0.0386s/iter; left time: 67.2812s
	iters: 500, epoch: 4 | loss: 0.1852838
	speed: 0.0391s/iter; left time: 64.3285s
Epoch: 4 cost time: 20.797897338867188
Epoch: 4, Steps: 536 | Train Loss: 0.2130735 Vali Loss: 0.3924627 Test Loss: 0.4543398
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : 3_2train_1val_2test_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 17257
test shape: (17257, 24, 16) (17257, 24, 16)
test shape: (17257, 24, 16) (17257, 24, 16)
model_id: 3_2train_1val_2test, mse:0.4162687361240387, mae:0.43381932377815247, rmse:0.6451889276504517 mape:2.998120069503784 mspe:19738.568359375
2025-02-11:23:34:09,876 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:23:34:09,876 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='4_1train_1val_3test', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=24, bucket_size=4, n_hashes=4, enc_in=16, dec_in=16, c_out=16, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=1, val_yrs=1, test_yrs=3)
Use GPU: cuda:0
>>>>>>>start training : 4_1train_1val_3test_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8521
val 8617
test 25897
	iters: 100, epoch: 1 | loss: 0.4237665
	speed: 0.0422s/iter; left time: 74.4586s
	iters: 200, epoch: 1 | loss: 0.4082463
	speed: 0.0359s/iter; left time: 59.7835s
Epoch: 1 cost time: 10.282392501831055
Epoch: 1, Steps: 266 | Train Loss: 0.4540092 Vali Loss: 0.4191692 Test Loss: 0.4636377
Validation loss decreased (inf --> 0.419169).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.3148239
	speed: 0.2868s/iter; left time: 429.3885s
	iters: 200, epoch: 2 | loss: 0.3370069
	speed: 0.0363s/iter; left time: 50.7657s
Epoch: 2 cost time: 11.126569271087646
Epoch: 2, Steps: 266 | Train Loss: 0.3353389 Vali Loss: 0.4500861 Test Loss: 0.4946069
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.3829647
	speed: 0.3001s/iter; left time: 369.4286s
	iters: 200, epoch: 3 | loss: 0.2580931
	speed: 0.0402s/iter; left time: 45.5084s
Epoch: 3 cost time: 11.04022479057312
Epoch: 3, Steps: 266 | Train Loss: 0.2697348 Vali Loss: 0.4660707 Test Loss: 0.5152841
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.2262862
	speed: 0.2982s/iter; left time: 287.7758s
	iters: 200, epoch: 4 | loss: 0.2251608
	speed: 0.0398s/iter; left time: 34.4269s
Epoch: 4 cost time: 10.740321397781372
Epoch: 4, Steps: 266 | Train Loss: 0.2401753 Vali Loss: 0.4719964 Test Loss: 0.5228736
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : 4_1train_1val_3test_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 25897
test shape: (25897, 24, 16) (25897, 24, 16)
test shape: (25897, 24, 16) (25897, 24, 16)
model_id: 4_1train_1val_3test, mse:0.46367374062538147, mae:0.4578571319580078, rmse:0.6809359192848206 mape:2.8486242294311523 mspe:10569.4775390625
2025-02-11:23:36:46,974 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:23:36:46,975 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='5_15cols', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=24, bucket_size=4, n_hashes=4, enc_in=15, dec_in=15, c_out=15, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=3, val_yrs=1, test_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 5_15cols_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 25801
val 8617
test 8617
Traceback (most recent call last):
  File "/home/hpc/Documents/autoformer-original/run.py", line 162, in <module>
    main()
  File "/home/hpc/Documents/autoformer-original/run.py", line 126, in main
    exp.train(setting)
  File "/home/hpc/Documents/autoformer-original/exp/exp_main.py", line 139, in train
    outputs, batch_y = self._predict(batch_x, batch_y, batch_x_mark, batch_y_mark)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/exp/exp_main.py", line 73, in _predict
    outputs = _run_model()
              ^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/exp/exp_main.py", line 64, in _run_model
    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/models/Autoformer.py", line 84, in forward
    enc_out = self.enc_embedding(x_enc, x_mark_enc)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/layers/Embed.py", line 158, in forward
    x = self.value_embedding(x) + self.temporal_embedding(x_mark)
        ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/layers/Embed.py", line 60, in forward
    x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 375, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 359, in _conv_forward
    return F.conv1d(
           ^^^^^^^^^
RuntimeError: Given groups=1, weight of size [512, 15, 3], expected input[32, 16, 98] to have 15 channels, but got 16 channels instead
2025-02-11:23:36:50,855 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:23:36:50,855 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='5_14cols', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=24, bucket_size=4, n_hashes=4, enc_in=14, dec_in=14, c_out=14, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=3, val_yrs=1, test_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 5_14cols_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 25801
val 8617
test 8617
Traceback (most recent call last):
  File "/home/hpc/Documents/autoformer-original/run.py", line 162, in <module>
    main()
  File "/home/hpc/Documents/autoformer-original/run.py", line 126, in main
    exp.train(setting)
  File "/home/hpc/Documents/autoformer-original/exp/exp_main.py", line 139, in train
    outputs, batch_y = self._predict(batch_x, batch_y, batch_x_mark, batch_y_mark)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/exp/exp_main.py", line 73, in _predict
    outputs = _run_model()
              ^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/exp/exp_main.py", line 64, in _run_model
    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/models/Autoformer.py", line 84, in forward
    enc_out = self.enc_embedding(x_enc, x_mark_enc)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/layers/Embed.py", line 158, in forward
    x = self.value_embedding(x) + self.temporal_embedding(x_mark)
        ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/layers/Embed.py", line 60, in forward
    x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 375, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 359, in _conv_forward
    return F.conv1d(
           ^^^^^^^^^
RuntimeError: Given groups=1, weight of size [512, 14, 3], expected input[32, 16, 98] to have 14 channels, but got 16 channels instead
2025-02-11:23:36:54,742 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:23:36:54,743 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='5_13cols', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=24, bucket_size=4, n_hashes=4, enc_in=13, dec_in=13, c_out=13, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=3, val_yrs=1, test_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 5_13cols_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 25801
val 8617
test 8617
Traceback (most recent call last):
  File "/home/hpc/Documents/autoformer-original/run.py", line 162, in <module>
    main()
  File "/home/hpc/Documents/autoformer-original/run.py", line 126, in main
    exp.train(setting)
  File "/home/hpc/Documents/autoformer-original/exp/exp_main.py", line 139, in train
    outputs, batch_y = self._predict(batch_x, batch_y, batch_x_mark, batch_y_mark)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/exp/exp_main.py", line 73, in _predict
    outputs = _run_model()
              ^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/exp/exp_main.py", line 64, in _run_model
    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/models/Autoformer.py", line 84, in forward
    enc_out = self.enc_embedding(x_enc, x_mark_enc)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/layers/Embed.py", line 158, in forward
    x = self.value_embedding(x) + self.temporal_embedding(x_mark)
        ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/layers/Embed.py", line 60, in forward
    x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 375, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 359, in _conv_forward
    return F.conv1d(
           ^^^^^^^^^
RuntimeError: Given groups=1, weight of size [512, 13, 3], expected input[32, 16, 98] to have 13 channels, but got 16 channels instead
2025-02-11:23:36:58,627 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:23:36:58,627 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='5_6cols', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=24, bucket_size=4, n_hashes=4, enc_in=6, dec_in=6, c_out=6, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=3, val_yrs=1, test_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 5_6cols_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 25801
val 8617
test 8617
Traceback (most recent call last):
  File "/home/hpc/Documents/autoformer-original/run.py", line 162, in <module>
    main()
  File "/home/hpc/Documents/autoformer-original/run.py", line 126, in main
    exp.train(setting)
  File "/home/hpc/Documents/autoformer-original/exp/exp_main.py", line 139, in train
    outputs, batch_y = self._predict(batch_x, batch_y, batch_x_mark, batch_y_mark)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/exp/exp_main.py", line 73, in _predict
    outputs = _run_model()
              ^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/exp/exp_main.py", line 64, in _run_model
    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/models/Autoformer.py", line 84, in forward
    enc_out = self.enc_embedding(x_enc, x_mark_enc)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/layers/Embed.py", line 158, in forward
    x = self.value_embedding(x) + self.temporal_embedding(x_mark)
        ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/layers/Embed.py", line 60, in forward
    x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 375, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 359, in _conv_forward
    return F.conv1d(
           ^^^^^^^^^
RuntimeError: Given groups=1, weight of size [512, 6, 3], expected input[32, 16, 98] to have 6 channels, but got 16 channels instead
2025-02-11:23:37:02,486 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:23:37:02,486 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='5_3cols', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=24, bucket_size=4, n_hashes=4, enc_in=3, dec_in=3, c_out=3, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=3, val_yrs=1, test_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 5_3cols_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 25801
val 8617
test 8617
Traceback (most recent call last):
  File "/home/hpc/Documents/autoformer-original/run.py", line 162, in <module>
    main()
  File "/home/hpc/Documents/autoformer-original/run.py", line 126, in main
    exp.train(setting)
  File "/home/hpc/Documents/autoformer-original/exp/exp_main.py", line 139, in train
    outputs, batch_y = self._predict(batch_x, batch_y, batch_x_mark, batch_y_mark)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/exp/exp_main.py", line 73, in _predict
    outputs = _run_model()
              ^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/exp/exp_main.py", line 64, in _run_model
    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/models/Autoformer.py", line 84, in forward
    enc_out = self.enc_embedding(x_enc, x_mark_enc)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/layers/Embed.py", line 158, in forward
    x = self.value_embedding(x) + self.temporal_embedding(x_mark)
        ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/layers/Embed.py", line 60, in forward
    x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 375, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 359, in _conv_forward
    return F.conv1d(
           ^^^^^^^^^
RuntimeError: Given groups=1, weight of size [512, 3, 3], expected input[32, 16, 98] to have 3 channels, but got 16 channels instead
2025-02-11:23:37:06,409 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:23:37:06,409 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='5_1col', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=24, bucket_size=4, n_hashes=4, enc_in=1, dec_in=1, c_out=1, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=3, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=3, val_yrs=1, test_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 5_1col_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 25801
val 8617
test 8617
Traceback (most recent call last):
  File "/home/hpc/Documents/autoformer-original/run.py", line 162, in <module>
    main()
  File "/home/hpc/Documents/autoformer-original/run.py", line 126, in main
    exp.train(setting)
  File "/home/hpc/Documents/autoformer-original/exp/exp_main.py", line 139, in train
    outputs, batch_y = self._predict(batch_x, batch_y, batch_x_mark, batch_y_mark)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/exp/exp_main.py", line 73, in _predict
    outputs = _run_model()
              ^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/exp/exp_main.py", line 64, in _run_model
    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/models/Autoformer.py", line 84, in forward
    enc_out = self.enc_embedding(x_enc, x_mark_enc)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/layers/Embed.py", line 158, in forward
    x = self.value_embedding(x) + self.temporal_embedding(x_mark)
        ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/Documents/autoformer-original/layers/Embed.py", line 60, in forward
    x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 375, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 359, in _conv_forward
    return F.conv1d(
           ^^^^^^^^^
RuntimeError: Given groups=1, weight of size [512, 1, 3], expected input[32, 16, 98] to have 1 channels, but got 16 channels instead
2025-02-11:23:37:10,345 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:23:37:10,345 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='6_2factor', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=24, bucket_size=4, n_hashes=4, enc_in=16, dec_in=16, c_out=16, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=2, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=3, val_yrs=1, test_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 6_2factor_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc2_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 25801
val 8617
test 8617
	iters: 100, epoch: 1 | loss: 0.3849958
	speed: 0.0386s/iter; left time: 213.7511s
	iters: 200, epoch: 1 | loss: 0.3560700
	speed: 0.0332s/iter; left time: 180.9610s
	iters: 300, epoch: 1 | loss: 0.3166476
	speed: 0.0344s/iter; left time: 183.6617s
	iters: 400, epoch: 1 | loss: 0.3661494
	speed: 0.0333s/iter; left time: 174.7338s
	iters: 500, epoch: 1 | loss: 0.2975256
	speed: 0.0337s/iter; left time: 173.2737s
	iters: 600, epoch: 1 | loss: 0.3397819
	speed: 0.0334s/iter; left time: 168.4467s
	iters: 700, epoch: 1 | loss: 0.3447456
	speed: 0.0344s/iter; left time: 170.0564s
	iters: 800, epoch: 1 | loss: 0.2995140
	speed: 0.0337s/iter; left time: 163.3551s
Epoch: 1 cost time: 27.774383783340454
Epoch: 1, Steps: 806 | Train Loss: 0.4033118 Vali Loss: 0.3448857 Test Loss: 0.3650116
Validation loss decreased (inf --> 0.344886).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.2901163
	speed: 0.1449s/iter; left time: 686.3181s
	iters: 200, epoch: 2 | loss: 0.3274311
	speed: 0.0335s/iter; left time: 155.3318s
	iters: 300, epoch: 2 | loss: 0.3365123
	speed: 0.0321s/iter; left time: 145.7483s
	iters: 400, epoch: 2 | loss: 0.2943645
	speed: 0.0323s/iter; left time: 143.2457s
	iters: 500, epoch: 2 | loss: 0.2202816
	speed: 0.0320s/iter; left time: 138.7714s
	iters: 600, epoch: 2 | loss: 0.3102962
	speed: 0.0319s/iter; left time: 135.3145s
	iters: 700, epoch: 2 | loss: 0.2772200
	speed: 0.0333s/iter; left time: 137.8154s
	iters: 800, epoch: 2 | loss: 0.3679295
	speed: 0.0316s/iter; left time: 127.6511s
Epoch: 2 cost time: 26.5876727104187
Epoch: 2, Steps: 806 | Train Loss: 0.3081067 Vali Loss: 0.3433428 Test Loss: 0.3695603
Validation loss decreased (0.344886 --> 0.343343).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.2458594
	speed: 0.1455s/iter; left time: 571.8217s
	iters: 200, epoch: 3 | loss: 0.2602595
	speed: 0.0354s/iter; left time: 135.4665s
	iters: 300, epoch: 3 | loss: 0.2308380
	speed: 0.0354s/iter; left time: 132.1057s
	iters: 400, epoch: 3 | loss: 0.2189975
	speed: 0.0386s/iter; left time: 140.2648s
	iters: 500, epoch: 3 | loss: 0.3018632
	speed: 0.0375s/iter; left time: 132.2653s
	iters: 600, epoch: 3 | loss: 0.2229271
	speed: 0.0357s/iter; left time: 122.4199s
	iters: 700, epoch: 3 | loss: 0.3026039
	speed: 0.0320s/iter; left time: 106.6323s
	iters: 800, epoch: 3 | loss: 0.2984971
	speed: 0.0315s/iter; left time: 101.7918s
Epoch: 3 cost time: 28.649646520614624
Epoch: 3, Steps: 806 | Train Loss: 0.2567466 Vali Loss: 0.3380076 Test Loss: 0.3642270
Validation loss decreased (0.343343 --> 0.338008).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.2009349
	speed: 0.1400s/iter; left time: 437.4436s
	iters: 200, epoch: 4 | loss: 0.2821882
	speed: 0.0335s/iter; left time: 101.4858s
	iters: 300, epoch: 4 | loss: 0.2070216
	speed: 0.0316s/iter; left time: 92.3159s
	iters: 400, epoch: 4 | loss: 0.2289022
	speed: 0.0318s/iter; left time: 89.8894s
	iters: 500, epoch: 4 | loss: 0.2163279
	speed: 0.0327s/iter; left time: 89.0229s
	iters: 600, epoch: 4 | loss: 0.2120449
	speed: 0.0389s/iter; left time: 102.0071s
	iters: 700, epoch: 4 | loss: 0.1937907
	speed: 0.0347s/iter; left time: 87.5214s
	iters: 800, epoch: 4 | loss: 0.2379135
	speed: 0.0390s/iter; left time: 94.6502s
Epoch: 4 cost time: 27.996127605438232
Epoch: 4, Steps: 806 | Train Loss: 0.2297306 Vali Loss: 0.3445040 Test Loss: 0.3657368
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.2037637
	speed: 0.1448s/iter; left time: 335.7868s
	iters: 200, epoch: 5 | loss: 0.2199470
	speed: 0.0355s/iter; left time: 78.7843s
	iters: 300, epoch: 5 | loss: 0.2179849
	speed: 0.0391s/iter; left time: 82.9043s
	iters: 400, epoch: 5 | loss: 0.2735990
	speed: 0.0376s/iter; left time: 75.8202s
	iters: 500, epoch: 5 | loss: 0.2426066
	speed: 0.0384s/iter; left time: 73.6907s
	iters: 600, epoch: 5 | loss: 0.1853211
	speed: 0.0402s/iter; left time: 73.0560s
	iters: 700, epoch: 5 | loss: 0.2468852
	speed: 0.0378s/iter; left time: 65.0036s
	iters: 800, epoch: 5 | loss: 0.1946552
	speed: 0.0330s/iter; left time: 53.5049s
Epoch: 5 cost time: 30.5756254196167
Epoch: 5, Steps: 806 | Train Loss: 0.2157229 Vali Loss: 0.3446700 Test Loss: 0.3716463
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.2113478
	speed: 0.1367s/iter; left time: 206.7807s
	iters: 200, epoch: 6 | loss: 0.2158369
	speed: 0.0321s/iter; left time: 45.3726s
	iters: 300, epoch: 6 | loss: 0.2040405
	speed: 0.0305s/iter; left time: 40.0424s
	iters: 400, epoch: 6 | loss: 0.1871761
	speed: 0.0304s/iter; left time: 36.8348s
	iters: 500, epoch: 6 | loss: 0.1942092
	speed: 0.0311s/iter; left time: 34.6189s
	iters: 600, epoch: 6 | loss: 0.2075094
	speed: 0.0304s/iter; left time: 30.8429s
	iters: 700, epoch: 6 | loss: 0.1795672
	speed: 0.0304s/iter; left time: 27.7887s
	iters: 800, epoch: 6 | loss: 0.2062818
	speed: 0.0309s/iter; left time: 25.1109s
Epoch: 6 cost time: 25.236557722091675
Epoch: 6, Steps: 806 | Train Loss: 0.2087145 Vali Loss: 0.3507841 Test Loss: 0.3757308
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : 6_2factor_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc2_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 8617
test shape: (8617, 24, 16) (8617, 24, 16)
test shape: (8617, 24, 16) (8617, 24, 16)
model_id: 6_2factor, mse:0.3647629916667938, mae:0.40504908561706543, rmse:0.6039561033248901 mape:2.716010093688965 mspe:4583.70751953125
2025-02-11:23:41:09,066 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:23:41:09,066 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='6_1factor', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=24, bucket_size=4, n_hashes=4, enc_in=16, dec_in=16, c_out=16, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=3, val_yrs=1, test_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 6_1factor_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 25801
val 8617
test 8617
	iters: 100, epoch: 1 | loss: 0.3854989
	speed: 0.0329s/iter; left time: 182.4048s
	iters: 200, epoch: 1 | loss: 0.3551815
	speed: 0.0280s/iter; left time: 152.6395s
	iters: 300, epoch: 1 | loss: 0.3020748
	speed: 0.0270s/iter; left time: 144.0289s
	iters: 400, epoch: 1 | loss: 0.3675089
	speed: 0.0269s/iter; left time: 141.2229s
	iters: 500, epoch: 1 | loss: 0.2918952
	speed: 0.0293s/iter; left time: 150.6111s
	iters: 600, epoch: 1 | loss: 0.3334019
	speed: 0.0292s/iter; left time: 147.3849s
	iters: 700, epoch: 1 | loss: 0.3390788
	speed: 0.0332s/iter; left time: 163.9575s
	iters: 800, epoch: 1 | loss: 0.2702115
	speed: 0.0314s/iter; left time: 152.1617s
Epoch: 1 cost time: 24.07476043701172
Epoch: 1, Steps: 806 | Train Loss: 0.3947190 Vali Loss: 0.3525625 Test Loss: 0.3679618
Validation loss decreased (inf --> 0.352563).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.2772563
	speed: 0.1288s/iter; left time: 610.0729s
	iters: 200, epoch: 2 | loss: 0.3076813
	speed: 0.0281s/iter; left time: 130.3066s
	iters: 300, epoch: 2 | loss: 0.3353014
	speed: 0.0273s/iter; left time: 123.8439s
	iters: 400, epoch: 2 | loss: 0.2925781
	speed: 0.0280s/iter; left time: 124.1612s
	iters: 500, epoch: 2 | loss: 0.2074219
	speed: 0.0273s/iter; left time: 118.2272s
	iters: 600, epoch: 2 | loss: 0.2963854
	speed: 0.0266s/iter; left time: 112.5056s
	iters: 700, epoch: 2 | loss: 0.2752332
	speed: 0.0260s/iter; left time: 107.4545s
	iters: 800, epoch: 2 | loss: 0.3713100
	speed: 0.0273s/iter; left time: 110.1725s
Epoch: 2 cost time: 22.580114364624023
Epoch: 2, Steps: 806 | Train Loss: 0.3012218 Vali Loss: 0.3516120 Test Loss: 0.3714427
Validation loss decreased (0.352563 --> 0.351612).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.2418768
	speed: 0.1238s/iter; left time: 486.5783s
	iters: 200, epoch: 3 | loss: 0.2626074
	speed: 0.0276s/iter; left time: 105.7627s
	iters: 300, epoch: 3 | loss: 0.2276031
	speed: 0.0271s/iter; left time: 101.1002s
	iters: 400, epoch: 3 | loss: 0.2345684
	speed: 0.0301s/iter; left time: 109.1547s
	iters: 500, epoch: 3 | loss: 0.2909915
	speed: 0.0275s/iter; left time: 97.1072s
	iters: 600, epoch: 3 | loss: 0.2272647
	speed: 0.0269s/iter; left time: 92.1292s
	iters: 700, epoch: 3 | loss: 0.3023819
	speed: 0.0269s/iter; left time: 89.6169s
	iters: 800, epoch: 3 | loss: 0.2902596
	speed: 0.0269s/iter; left time: 86.7952s
Epoch: 3 cost time: 22.631503343582153
Epoch: 3, Steps: 806 | Train Loss: 0.2521985 Vali Loss: 0.3677409 Test Loss: 0.3859368
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.2031552
	speed: 0.1243s/iter; left time: 388.4644s
	iters: 200, epoch: 4 | loss: 0.2905394
	speed: 0.0300s/iter; left time: 90.6062s
	iters: 300, epoch: 4 | loss: 0.2036721
	speed: 0.0277s/iter; left time: 81.0374s
	iters: 400, epoch: 4 | loss: 0.2037278
	speed: 0.0286s/iter; left time: 80.8741s
	iters: 500, epoch: 4 | loss: 0.2180820
	speed: 0.0280s/iter; left time: 76.1696s
	iters: 600, epoch: 4 | loss: 0.2003064
	speed: 0.0306s/iter; left time: 80.4389s
	iters: 700, epoch: 4 | loss: 0.1913953
	speed: 0.0313s/iter; left time: 79.0821s
	iters: 800, epoch: 4 | loss: 0.2354596
	speed: 0.0305s/iter; left time: 73.9502s
Epoch: 4 cost time: 24.231000661849976
Epoch: 4, Steps: 806 | Train Loss: 0.2244531 Vali Loss: 0.3706322 Test Loss: 0.3917844
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.2112093
	speed: 0.1238s/iter; left time: 287.1480s
	iters: 200, epoch: 5 | loss: 0.2114348
	speed: 0.0306s/iter; left time: 67.9447s
	iters: 300, epoch: 5 | loss: 0.2089822
	speed: 0.0312s/iter; left time: 66.0604s
	iters: 400, epoch: 5 | loss: 0.2458632
	speed: 0.0272s/iter; left time: 54.8389s
	iters: 500, epoch: 5 | loss: 0.2366519
	speed: 0.0272s/iter; left time: 52.2059s
	iters: 600, epoch: 5 | loss: 0.1673746
	speed: 0.0270s/iter; left time: 49.0466s
	iters: 700, epoch: 5 | loss: 0.2259663
	speed: 0.0269s/iter; left time: 46.2619s
	iters: 800, epoch: 5 | loss: 0.1962047
	speed: 0.0328s/iter; left time: 53.1406s
Epoch: 5 cost time: 23.481341123580933
Epoch: 5, Steps: 806 | Train Loss: 0.2094850 Vali Loss: 0.3842649 Test Loss: 0.4122896
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : 6_1factor_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 8617
test shape: (8617, 24, 16) (8617, 24, 16)
test shape: (8617, 24, 16) (8617, 24, 16)
model_id: 6_1factor, mse:0.37159496545791626, mae:0.412276953458786, rmse:0.6095858812332153 mape:2.7613368034362793 mspe:4795.078125
2025-02-11:23:44:01,394 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-02-11:23:44:01,394 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
Args in experiment:
Namespace(is_training=1, model_id='6_5factor', model='Autoformer', data='Energy', root_path='./dataset/Energy/', data_path='load_forecasting.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=24, bucket_size=4, n_hashes=4, enc_in=16, dec_in=16, c_out=16, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=7, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', train_yrs=3, val_yrs=1, test_yrs=1)
Use GPU: cuda:0
>>>>>>>start training : 6_5factor_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 25801
val 8617
test 8617
	iters: 100, epoch: 1 | loss: 0.3788764
	speed: 0.0567s/iter; left time: 314.3989s
	iters: 200, epoch: 1 | loss: 0.3461429
	speed: 0.0480s/iter; left time: 261.2976s
	iters: 300, epoch: 1 | loss: 0.3018271
	speed: 0.0499s/iter; left time: 266.7796s
	iters: 400, epoch: 1 | loss: 0.3566835
	speed: 0.0503s/iter; left time: 263.5743s
	iters: 500, epoch: 1 | loss: 0.2945122
	speed: 0.0508s/iter; left time: 261.5076s
	iters: 600, epoch: 1 | loss: 0.3338395
	speed: 0.0485s/iter; left time: 244.8005s
	iters: 700, epoch: 1 | loss: 0.3279625
	speed: 0.0478s/iter; left time: 236.0916s
	iters: 800, epoch: 1 | loss: 0.2847106
	speed: 0.0474s/iter; left time: 229.6544s
Epoch: 1 cost time: 40.304805755615234
Epoch: 1, Steps: 806 | Train Loss: 0.3891943 Vali Loss: 0.3337778 Test Loss: 0.3550816
Validation loss decreased (inf --> 0.333778).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.2844231
	speed: 0.1948s/iter; left time: 922.7955s
	iters: 200, epoch: 2 | loss: 0.2999752
	speed: 0.0502s/iter; left time: 232.6136s
	iters: 300, epoch: 2 | loss: 0.2959293
	speed: 0.0501s/iter; left time: 227.4335s
	iters: 400, epoch: 2 | loss: 0.2802874
	speed: 0.0500s/iter; left time: 222.0643s
	iters: 500, epoch: 2 | loss: 0.1919376
	speed: 0.0494s/iter; left time: 214.1601s
	iters: 600, epoch: 2 | loss: 0.2978077
	speed: 0.0490s/iter; left time: 207.7522s
	iters: 700, epoch: 2 | loss: 0.2584835
	speed: 0.0482s/iter; left time: 199.4306s
	iters: 800, epoch: 2 | loss: 0.3417739
	speed: 0.0481s/iter; left time: 194.0464s
Epoch: 2 cost time: 40.033629417419434
Epoch: 2, Steps: 806 | Train Loss: 0.2878656 Vali Loss: 0.3381750 Test Loss: 0.3792035
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.2390706
	speed: 0.1969s/iter; left time: 773.8722s
	iters: 200, epoch: 3 | loss: 0.2418701
	speed: 0.0493s/iter; left time: 188.8218s
	iters: 300, epoch: 3 | loss: 0.2098024
	speed: 0.0478s/iter; left time: 178.3460s
	iters: 400, epoch: 3 | loss: 0.2096790
	speed: 0.0477s/iter; left time: 173.1235s
	iters: 500, epoch: 3 | loss: 0.2768117
	speed: 0.0495s/iter; left time: 174.9107s
	iters: 600, epoch: 3 | loss: 0.2104248
	speed: 0.0483s/iter; left time: 165.8528s
	iters: 700, epoch: 3 | loss: 0.2968020
	speed: 0.0477s/iter; left time: 158.8153s
	iters: 800, epoch: 3 | loss: 0.2653096
	speed: 0.0482s/iter; left time: 155.8886s
Epoch: 3 cost time: 39.49749231338501
Epoch: 3, Steps: 806 | Train Loss: 0.2384258 Vali Loss: 0.3380217 Test Loss: 0.3763594
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.1896705
	speed: 0.1945s/iter; left time: 607.9248s
	iters: 200, epoch: 4 | loss: 0.2627195
	speed: 0.0489s/iter; left time: 147.8243s
	iters: 300, epoch: 4 | loss: 0.1860588
	speed: 0.0492s/iter; left time: 143.9554s
	iters: 400, epoch: 4 | loss: 0.2029105
	speed: 0.0506s/iter; left time: 142.8203s
	iters: 500, epoch: 4 | loss: 0.1906204
	speed: 0.0493s/iter; left time: 134.3839s
	iters: 600, epoch: 4 | loss: 0.1978298
	speed: 0.0497s/iter; left time: 130.4854s
	iters: 700, epoch: 4 | loss: 0.1823542
	speed: 0.0487s/iter; left time: 123.0330s
	iters: 800, epoch: 4 | loss: 0.2179525
	speed: 0.0484s/iter; left time: 117.3762s
Epoch: 4 cost time: 40.0349657535553
Epoch: 4, Steps: 806 | Train Loss: 0.2130552 Vali Loss: 0.3454386 Test Loss: 0.3886556
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : 6_5factor_Autoformer_Energy_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 8617
test shape: (8617, 24, 16) (8617, 24, 16)
test shape: (8617, 24, 16) (8617, 24, 16)
model_id: 6_5factor, mse:0.35554736852645874, mae:0.39095577597618103, rmse:0.5962779521942139 mape:2.5095694065093994 mspe:3259.9599609375

[Done] exited with code=0 in 3770.838 seconds

